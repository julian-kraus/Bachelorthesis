{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# German Llama fine-tuned with mlx for diagnosis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Define the number of samples per label\n",
    "samples_per_label = 17"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test DataFrame:\n",
      "                                                prompt  \\\n",
      "0    Seit ca. 6 Jahren Shcmerzen re. Kniegelenk; Be...   \n",
      "1    2019 erstmalig nach Ergometertrainiung Schmerz...   \n",
      "2    Damals Judo und Kickboxen gemacht; kein größer...   \n",
      "3    Vor 15 Jahren erstmalige Patellaluxation links...   \n",
      "4    z.n Patellaluxation 1992; seit vielen Jahren K...   \n",
      "..                                                 ...   \n",
      "97   Ende 08/12 beim Fußall Distorsionstrauma ohne ...   \n",
      "98   Z.n. LCA-Ersatz mit 4x ST 2017 bei Jürgen.  Da...   \n",
      "99   Seit 25 Jahren Kampfsport; vor 12 Jahren Schme...   \n",
      "100  Am 10.10.10 Kreuzbandriss beim Fußballspiel; P...   \n",
      "101  seit Januar Schmerzen rechtes Knie ohne erinne...   \n",
      "\n",
      "                                            completion  \n",
      "0                        Aktivierte Gonarthrose rechts  \n",
      "1        Krankheit der Patella; nicht näher bezeichnet  \n",
      "2                              Chondromalacia patellae  \n",
      "3                      Habituelle Luxation der Patella  \n",
      "4            funktionelles Patellaspitzensyndrom links  \n",
      "..                                                 ...  \n",
      "97   Sonstige Binnenschädigungen des Kniegelenkes: ...  \n",
      "98   Sonstige Binnenschädigungen des Kniegelenkes: ...  \n",
      "99   Sonstige Binnenschädigungen des Kniegelenkes: ...  \n",
      "100  Sonstige Binnenschädigungen des Kniegelenkes: ...  \n",
      "101  IV° Chondromalacie retropatellar rechtes Knieg...  \n",
      "\n",
      "[102 rows x 2 columns]\n",
      "\n",
      "Train DataFrame:\n",
      "                                                  prompt  \\\n",
      "0      Seit ca 2008 Schmerzen im linken Knie.  Im Jun...   \n",
      "1      Vor 14 Tagen Sturz beim Skifahren in Österreic...   \n",
      "2      2002 LCA Ruptur rechts mit anschl. Ersatz mit ...   \n",
      "3      MRT bei DR Buckup wegen Schmerzen rechtes Knie...   \n",
      "4      Am 03.12.2020 bei einem Ausfallschritt plötzli...   \n",
      "...                                                  ...   \n",
      "36116  Distorsionstrauma rechtes Kniegelenk beim Fußb...   \n",
      "36117  Knieschmerzen links seit 12.2.04. Beim Schifah...   \n",
      "36118  Bei Ultimate Frisbee vor ca 3 Wochen das re. K...   \n",
      "36119  Seit einigen Monaten Beschwerden im re Knie; k...   \n",
      "36120  Beschwerden mit beiden Kniescheiben seit ca 5 ...   \n",
      "\n",
      "                                              completion  \n",
      "0                    Gonarthrose; nicht näher bezeichnet  \n",
      "1      Verstauchung und Zerrung sonstiger und nicht n...  \n",
      "2                    Gonarthrose; nicht näher bezeichnet  \n",
      "3         III° Chondromalazie retropatellar rechtes Knie  \n",
      "4                   V.a. Freier Gelenkkörper Knie rechts  \n",
      "...                                                  ...  \n",
      "36116  Sonstige Binnenschädigungen des Kniegelenkes: ...  \n",
      "36117  Sonstige Binnenschädigungen des Kniegelenkes: ...  \n",
      "36118  Sonstige Binnenschädigungen des Kniegelenkes: ...  \n",
      "36119       Sonstige Binnenschädigungen des Kniegelenkes  \n",
      "36120  Sonstige Binnenschädigungen des Kniegelenkes: ...  \n",
      "\n",
      "[36121 rows x 2 columns]\n",
      "\n",
      "Valid DataFrame:\n",
      "                                                 prompt  \\\n",
      "0     seit 8 Jahren Schmerzen im re Knie bes. in Ruh...   \n",
      "1     Zum fünften Mal Patellaluxation 07/22 beim Fuß...   \n",
      "2     Z.n. Skiunfall mit LCA Rx 30.12.2009; LCA-Ersa...   \n",
      "3     Seit 1 Jahr Schmerzen li Knie; kein Trauma; su...   \n",
      "4     Z.n AC re. Knie mit IM-Teilresektion 3/05 in K...   \n",
      "...                                                 ...   \n",
      "9029  Schmerzen und Schwellung im re Knie nach Proth...   \n",
      "9030  damals keine Instabilität festgestellt; voe 4 ...   \n",
      "9031  Seit Jahren Kniebeschwerden bds.. Aktuell rech...   \n",
      "9032  Seit ca. 2 Monaten Schmerzen und Bewegungseins...   \n",
      "9033  Pat. habe vor 6 Tagen beim Vorzeigen einer Jud...   \n",
      "\n",
      "                                             completion  \n",
      "0                               V.a.freien Gelenkkörper  \n",
      "1                       Habituelle Luxation der Patella  \n",
      "2     Verstauchung und Zerrung sonstiger und nicht n...  \n",
      "3                     medial betonte Gonarthrose  links  \n",
      "4                  Krankheiten im Patellofemoralbereich  \n",
      "...                                                 ...  \n",
      "9029  Sonstige Binnenschädigungen des Kniegelenkes: ...  \n",
      "9030  Sonstige Binnenschädigungen des Kniegelenkes: ...  \n",
      "9031  Sonstige Binnenschädigungen des Kniegelenkes: ...  \n",
      "9032  Sonstige Binnenschädigungen des Kniegelenkes: ...  \n",
      "9033  Sonstige Binnenschädigungen des Kniegelenkes: ...  \n",
      "\n",
      "[9034 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# Load and preprocess the dataset\n",
    "df = pd.read_csv('../../data/balanced_classification_dataset.csv')\n",
    "df = df.rename(columns={'data': 'prompt', 'DIA_text': 'completion'})\n",
    "df = df.drop(df.columns.difference(['prompt', 'completion', 'label']), axis=1)\n",
    "\n",
    "# Switch the places of 'prompt' and 'completion' columns\n",
    "columns = list(df.columns)\n",
    "col1, col2 = columns.index('prompt'), columns.index('completion')\n",
    "columns[col1], columns[col2] = columns[col2], columns[col1]\n",
    "df = df[columns]\n",
    "\n",
    "# Create the test DataFrame\n",
    "samples_per_label = 17\n",
    "test = pd.DataFrame()\n",
    "\n",
    "for label in df['label'].unique():\n",
    "    label_subset = df[df['label'] == label].sample(n=samples_per_label, random_state=42)\n",
    "    test = pd.concat([test, label_subset])\n",
    "\n",
    "# Remove 'label' column and reset index for the test DataFrame\n",
    "\n",
    "test.reset_index(drop=True, inplace=True)\n",
    "test = test.drop([\"label\"], axis=1)\n",
    "\n",
    "# Create the train DataFrame by dropping test rows\n",
    "train = df.drop(test.index)\n",
    "\n",
    "# Split the train DataFrame into train and valid DataFrames with 80:20 ratio\n",
    "train_df_list = []\n",
    "valid_df_list = []\n",
    "\n",
    "for label in train['label'].unique():\n",
    "    label_df = train[train['label'] == label]\n",
    "    train_label, valid_label = train_test_split(label_df, test_size=0.2, random_state=42, stratify=label_df[\"label\"])\n",
    "    train_df_list.append(train_label)\n",
    "    valid_df_list.append(valid_label)\n",
    "\n",
    "train = pd.concat(train_df_list).drop([\"label\"], axis=1).reset_index(drop=True)\n",
    "valid = pd.concat(valid_df_list).drop([\"label\"], axis=1).reset_index(drop=True)\n",
    "\n",
    "# Save DataFrames to CSV and JSONL formats\n",
    "test.to_csv(\"test.csv\", index=False)\n",
    "train.to_csv(\"train.csv\", index=False)\n",
    "valid.to_csv(\"valid.csv\", index=False)\n",
    "\n",
    "train.to_json(\"train.jsonl\", orient='records', lines=True)\n",
    "valid.to_json(\"valid.jsonl\", orient='records', lines=True)\n",
    "test.to_json(\"test.jsonl\", orient='records', lines=True)\n",
    "\n",
    "# Display the result\n",
    "print(\"Test DataFrame:\")\n",
    "print(test)\n",
    "print(\"\\nTrain DataFrame:\")\n",
    "print(train)\n",
    "print(\"\\nValid DataFrame:\")\n",
    "print(valid)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Loading\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 10 files: 100%|██████████| 10/10 [00:00<00:00, 28093.13it/s]\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Quantizing\n"
     ]
    }
   ],
   "source": [
    "# %%bash\n",
    "# python -m mlx_lm.convert --hf-path DiscoResearch/Llama3-DiscoLeo-Instruct-8B-v0.1 --mlx-path models_mlx/ -q "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usage: mlx_lm.lora [-h] [--model MODEL] [--train] [--data DATA]\n",
      "                   [--lora-layers LORA_LAYERS] [--batch-size BATCH_SIZE]\n",
      "                   [--iters ITERS] [--val-batches VAL_BATCHES]\n",
      "                   [--learning-rate LEARNING_RATE]\n",
      "                   [--steps-per-report STEPS_PER_REPORT]\n",
      "                   [--steps-per-eval STEPS_PER_EVAL]\n",
      "                   [--resume-adapter-file RESUME_ADAPTER_FILE]\n",
      "                   [--adapter-path ADAPTER_PATH] [--save-every SAVE_EVERY]\n",
      "                   [--test] [--test-batches TEST_BATCHES]\n",
      "                   [--max-seq-length MAX_SEQ_LENGTH] [-c CONFIG]\n",
      "                   [--grad-checkpoint] [--seed SEED] [--use-dora]\n",
      "\n",
      "LoRA or QLoRA finetuning.\n",
      "\n",
      "options:\n",
      "  -h, --help            show this help message and exit\n",
      "  --model MODEL         The path to the local model directory or Hugging Face\n",
      "                        repo.\n",
      "  --train               Do training\n",
      "  --data DATA           Directory with {train, valid, test}.jsonl files\n",
      "  --lora-layers LORA_LAYERS\n",
      "                        Number of layers to fine-tune. Default is 16, use -1\n",
      "                        for all.\n",
      "  --batch-size BATCH_SIZE\n",
      "                        Minibatch size.\n",
      "  --iters ITERS         Iterations to train for.\n",
      "  --val-batches VAL_BATCHES\n",
      "                        Number of validation batches, -1 uses the entire\n",
      "                        validation set.\n",
      "  --learning-rate LEARNING_RATE\n",
      "                        Adam learning rate.\n",
      "  --steps-per-report STEPS_PER_REPORT\n",
      "                        Number of training steps between loss reporting.\n",
      "  --steps-per-eval STEPS_PER_EVAL\n",
      "                        Number of training steps between validations.\n",
      "  --resume-adapter-file RESUME_ADAPTER_FILE\n",
      "                        Load path to resume training with the given adapters.\n",
      "  --adapter-path ADAPTER_PATH\n",
      "                        Save/load path for the adapters.\n",
      "  --save-every SAVE_EVERY\n",
      "                        Save the model every N iterations.\n",
      "  --test                Evaluate on the test set after training\n",
      "  --test-batches TEST_BATCHES\n",
      "                        Number of test set batches, -1 uses the entire test\n",
      "                        set.\n",
      "  --max-seq-length MAX_SEQ_LENGTH\n",
      "                        Maximum sequence length.\n",
      "  -c CONFIG, --config CONFIG\n",
      "                        A YAML configuration file with the training options\n",
      "  --grad-checkpoint     Use gradient checkpointing to reduce memory use.\n",
      "  --seed SEED           The PRNG seed\n",
      "  --use-dora            Use DoRA to finetune.\n"
     ]
    }
   ],
   "source": [
    "# %%bash\n",
    "# mlx_lm.lora --help"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading datasets\n",
      "Training\n",
      "Trainable parameters: 0.021% (1.704M/8030.261M)\n",
      "Starting training..., iters: 1000\n",
      "Iter 1: Val loss 3.744, Val took 41.189s\n",
      "Iter 10: Train loss 3.659, Learning Rate 1.000e-05, It/sec 0.348, Tokens/sec 322.616, Trained Tokens 9259, Peak mem 12.686 GB\n",
      "Iter 20: Train loss 3.635, Learning Rate 1.000e-05, It/sec 0.381, Tokens/sec 328.917, Trained Tokens 17882, Peak mem 12.686 GB\n",
      "Iter 30: Train loss 2.993, Learning Rate 1.000e-05, It/sec 0.314, Tokens/sec 325.324, Trained Tokens 28238, Peak mem 13.213 GB\n",
      "Iter 40: Train loss 2.960, Learning Rate 1.000e-05, It/sec 0.358, Tokens/sec 338.686, Trained Tokens 37701, Peak mem 14.804 GB\n",
      "Iter 50: Train loss 2.822, Learning Rate 1.000e-05, It/sec 0.373, Tokens/sec 330.531, Trained Tokens 46563, Peak mem 14.804 GB\n",
      "Iter 60: Train loss 2.600, Learning Rate 1.000e-05, It/sec 0.303, Tokens/sec 309.994, Trained Tokens 56784, Peak mem 20.961 GB\n",
      "Iter 70: Train loss 2.340, Learning Rate 1.000e-05, It/sec 0.291, Tokens/sec 266.466, Trained Tokens 65935, Peak mem 20.961 GB\n",
      "Iter 80: Train loss 2.468, Learning Rate 1.000e-05, It/sec 0.239, Tokens/sec 224.186, Trained Tokens 75316, Peak mem 20.961 GB\n",
      "Iter 90: Train loss 2.312, Learning Rate 1.000e-05, It/sec 0.231, Tokens/sec 201.523, Trained Tokens 84039, Peak mem 20.961 GB\n",
      "Iter 100: Train loss 2.482, Learning Rate 1.000e-05, It/sec 0.268, Tokens/sec 219.302, Trained Tokens 92217, Peak mem 20.961 GB\n",
      "Iter 100: Saved adapter weights to adapters/adapters.safetensors and adapters/0000100_adapters.safetensors.\n",
      "Iter 110: Train loss 2.185, Learning Rate 1.000e-05, It/sec 0.286, Tokens/sec 232.432, Trained Tokens 100355, Peak mem 20.961 GB\n",
      "Iter 120: Train loss 2.287, Learning Rate 1.000e-05, It/sec 0.288, Tokens/sec 234.256, Trained Tokens 108484, Peak mem 20.961 GB\n",
      "Iter 130: Train loss 2.279, Learning Rate 1.000e-05, It/sec 0.298, Tokens/sec 252.137, Trained Tokens 116946, Peak mem 20.961 GB\n",
      "Iter 140: Train loss 2.112, Learning Rate 1.000e-05, It/sec 0.358, Tokens/sec 268.865, Trained Tokens 124448, Peak mem 20.961 GB\n",
      "Iter 150: Train loss 2.109, Learning Rate 1.000e-05, It/sec 0.344, Tokens/sec 290.438, Trained Tokens 132889, Peak mem 20.961 GB\n",
      "Iter 160: Train loss 2.079, Learning Rate 1.000e-05, It/sec 0.355, Tokens/sec 277.893, Trained Tokens 140719, Peak mem 20.961 GB\n",
      "Iter 170: Train loss 2.192, Learning Rate 1.000e-05, It/sec 0.288, Tokens/sec 288.808, Trained Tokens 150742, Peak mem 20.961 GB\n",
      "Iter 180: Train loss 2.162, Learning Rate 1.000e-05, It/sec 0.345, Tokens/sec 289.347, Trained Tokens 159125, Peak mem 20.961 GB\n",
      "Iter 190: Train loss 2.137, Learning Rate 1.000e-05, It/sec 0.308, Tokens/sec 288.407, Trained Tokens 168485, Peak mem 20.961 GB\n",
      "Iter 200: Val loss 2.060, Val took 56.347s\n",
      "Iter 200: Train loss 2.016, Learning Rate 1.000e-05, It/sec 3.938, Tokens/sec 3255.756, Trained Tokens 176752, Peak mem 20.961 GB\n",
      "Iter 200: Saved adapter weights to adapters/adapters.safetensors and adapters/0000200_adapters.safetensors.\n",
      "Iter 210: Train loss 2.016, Learning Rate 1.000e-05, It/sec 0.268, Tokens/sec 262.678, Trained Tokens 186550, Peak mem 20.961 GB\n",
      "Iter 220: Train loss 2.289, Learning Rate 1.000e-05, It/sec 0.308, Tokens/sec 255.276, Trained Tokens 194848, Peak mem 20.961 GB\n",
      "Iter 230: Train loss 2.137, Learning Rate 1.000e-05, It/sec 0.324, Tokens/sec 237.757, Trained Tokens 202195, Peak mem 20.961 GB\n",
      "Iter 240: Train loss 2.105, Learning Rate 1.000e-05, It/sec 0.317, Tokens/sec 264.986, Trained Tokens 210552, Peak mem 20.961 GB\n",
      "Iter 250: Train loss 2.191, Learning Rate 1.000e-05, It/sec 0.289, Tokens/sec 248.417, Trained Tokens 219151, Peak mem 20.961 GB\n",
      "Iter 260: Train loss 2.044, Learning Rate 1.000e-05, It/sec 0.318, Tokens/sec 264.690, Trained Tokens 227470, Peak mem 20.961 GB\n",
      "Iter 270: Train loss 2.152, Learning Rate 1.000e-05, It/sec 0.273, Tokens/sec 272.552, Trained Tokens 237443, Peak mem 20.961 GB\n",
      "Iter 280: Train loss 1.854, Learning Rate 1.000e-05, It/sec 0.318, Tokens/sec 276.568, Trained Tokens 246140, Peak mem 20.961 GB\n",
      "Iter 290: Train loss 2.005, Learning Rate 1.000e-05, It/sec 0.267, Tokens/sec 265.637, Trained Tokens 256072, Peak mem 20.961 GB\n",
      "Iter 300: Train loss 1.885, Learning Rate 1.000e-05, It/sec 0.309, Tokens/sec 274.527, Trained Tokens 264969, Peak mem 20.961 GB\n",
      "Iter 300: Saved adapter weights to adapters/adapters.safetensors and adapters/0000300_adapters.safetensors.\n",
      "Iter 310: Train loss 1.905, Learning Rate 1.000e-05, It/sec 0.287, Tokens/sec 267.855, Trained Tokens 274288, Peak mem 20.961 GB\n",
      "Iter 320: Train loss 1.930, Learning Rate 1.000e-05, It/sec 0.327, Tokens/sec 273.040, Trained Tokens 282647, Peak mem 20.961 GB\n",
      "Iter 330: Train loss 1.950, Learning Rate 1.000e-05, It/sec 0.289, Tokens/sec 265.009, Trained Tokens 291805, Peak mem 20.961 GB\n",
      "Iter 340: Train loss 2.100, Learning Rate 1.000e-05, It/sec 0.307, Tokens/sec 266.213, Trained Tokens 300489, Peak mem 20.961 GB\n",
      "Iter 350: Train loss 1.965, Learning Rate 1.000e-05, It/sec 0.330, Tokens/sec 264.044, Trained Tokens 308491, Peak mem 20.961 GB\n",
      "Iter 360: Train loss 1.911, Learning Rate 1.000e-05, It/sec 0.270, Tokens/sec 273.258, Trained Tokens 318617, Peak mem 20.961 GB\n",
      "Iter 370: Train loss 2.083, Learning Rate 1.000e-05, It/sec 0.263, Tokens/sec 268.868, Trained Tokens 328837, Peak mem 20.961 GB\n",
      "Iter 380: Train loss 2.094, Learning Rate 1.000e-05, It/sec 0.321, Tokens/sec 274.255, Trained Tokens 337393, Peak mem 20.961 GB\n",
      "Iter 390: Train loss 2.033, Learning Rate 1.000e-05, It/sec 0.283, Tokens/sec 262.046, Trained Tokens 346645, Peak mem 20.961 GB\n",
      "Iter 400: Val loss 1.897, Val took 52.296s\n",
      "Iter 400: Train loss 2.047, Learning Rate 1.000e-05, It/sec 2.822, Tokens/sec 2881.747, Trained Tokens 356855, Peak mem 20.961 GB\n",
      "Iter 400: Saved adapter weights to adapters/adapters.safetensors and adapters/0000400_adapters.safetensors.\n",
      "Iter 410: Train loss 1.955, Learning Rate 1.000e-05, It/sec 0.241, Tokens/sec 258.519, Trained Tokens 367591, Peak mem 20.961 GB\n",
      "Iter 420: Train loss 1.917, Learning Rate 1.000e-05, It/sec 0.311, Tokens/sec 269.529, Trained Tokens 376263, Peak mem 20.961 GB\n",
      "Iter 430: Train loss 1.986, Learning Rate 1.000e-05, It/sec 0.330, Tokens/sec 277.400, Trained Tokens 384669, Peak mem 20.961 GB\n",
      "Iter 440: Train loss 1.858, Learning Rate 1.000e-05, It/sec 0.310, Tokens/sec 273.298, Trained Tokens 393484, Peak mem 20.961 GB\n",
      "Iter 450: Train loss 1.935, Learning Rate 1.000e-05, It/sec 0.264, Tokens/sec 268.419, Trained Tokens 403639, Peak mem 20.961 GB\n",
      "Iter 460: Train loss 1.932, Learning Rate 1.000e-05, It/sec 0.303, Tokens/sec 270.169, Trained Tokens 412557, Peak mem 20.961 GB\n",
      "Iter 470: Train loss 1.919, Learning Rate 1.000e-05, It/sec 0.297, Tokens/sec 275.386, Trained Tokens 421821, Peak mem 20.961 GB\n",
      "Iter 480: Train loss 2.004, Learning Rate 1.000e-05, It/sec 0.298, Tokens/sec 274.428, Trained Tokens 431045, Peak mem 20.961 GB\n",
      "Iter 490: Train loss 1.924, Learning Rate 1.000e-05, It/sec 0.368, Tokens/sec 284.779, Trained Tokens 438791, Peak mem 20.961 GB\n",
      "Iter 500: Train loss 1.927, Learning Rate 1.000e-05, It/sec 0.325, Tokens/sec 282.131, Trained Tokens 447467, Peak mem 20.961 GB\n",
      "Iter 500: Saved adapter weights to adapters/adapters.safetensors and adapters/0000500_adapters.safetensors.\n",
      "Iter 510: Train loss 1.957, Learning Rate 1.000e-05, It/sec 0.327, Tokens/sec 276.610, Trained Tokens 455916, Peak mem 20.961 GB\n",
      "Iter 520: Train loss 1.853, Learning Rate 1.000e-05, It/sec 0.309, Tokens/sec 286.386, Trained Tokens 465190, Peak mem 20.961 GB\n",
      "Iter 530: Train loss 2.088, Learning Rate 1.000e-05, It/sec 0.369, Tokens/sec 275.522, Trained Tokens 472663, Peak mem 20.961 GB\n",
      "Iter 540: Train loss 2.027, Learning Rate 1.000e-05, It/sec 0.284, Tokens/sec 281.156, Trained Tokens 482559, Peak mem 20.961 GB\n",
      "Iter 550: Train loss 1.931, Learning Rate 1.000e-05, It/sec 0.341, Tokens/sec 285.069, Trained Tokens 490911, Peak mem 20.961 GB\n",
      "Iter 560: Train loss 1.884, Learning Rate 1.000e-05, It/sec 0.385, Tokens/sec 277.875, Trained Tokens 498130, Peak mem 20.961 GB\n",
      "Iter 570: Train loss 1.850, Learning Rate 1.000e-05, It/sec 0.307, Tokens/sec 265.641, Trained Tokens 506785, Peak mem 20.961 GB\n",
      "Iter 580: Train loss 1.869, Learning Rate 1.000e-05, It/sec 0.288, Tokens/sec 262.721, Trained Tokens 515920, Peak mem 20.961 GB\n",
      "Iter 590: Train loss 1.975, Learning Rate 1.000e-05, It/sec 0.273, Tokens/sec 272.540, Trained Tokens 525921, Peak mem 20.961 GB\n",
      "Iter 600: Val loss 1.896, Val took 44.771s\n",
      "Iter 600: Train loss 1.814, Learning Rate 1.000e-05, It/sec 4.998, Tokens/sec 4652.023, Trained Tokens 535229, Peak mem 20.961 GB\n",
      "Iter 600: Saved adapter weights to adapters/adapters.safetensors and adapters/0000600_adapters.safetensors.\n",
      "Iter 610: Train loss 1.972, Learning Rate 1.000e-05, It/sec 0.339, Tokens/sec 285.960, Trained Tokens 543656, Peak mem 20.961 GB\n",
      "Iter 620: Train loss 1.871, Learning Rate 1.000e-05, It/sec 0.339, Tokens/sec 288.460, Trained Tokens 552167, Peak mem 20.961 GB\n",
      "Iter 630: Train loss 1.879, Learning Rate 1.000e-05, It/sec 0.291, Tokens/sec 279.178, Trained Tokens 561771, Peak mem 20.961 GB\n",
      "Iter 640: Train loss 1.989, Learning Rate 1.000e-05, It/sec 0.364, Tokens/sec 288.594, Trained Tokens 569689, Peak mem 20.961 GB\n",
      "Iter 650: Train loss 1.877, Learning Rate 1.000e-05, It/sec 0.302, Tokens/sec 273.085, Trained Tokens 578742, Peak mem 20.961 GB\n",
      "Iter 660: Train loss 1.884, Learning Rate 1.000e-05, It/sec 0.368, Tokens/sec 277.954, Trained Tokens 586301, Peak mem 20.961 GB\n",
      "Iter 670: Train loss 1.905, Learning Rate 1.000e-05, It/sec 0.304, Tokens/sec 280.514, Trained Tokens 595538, Peak mem 20.961 GB\n",
      "Iter 680: Train loss 1.836, Learning Rate 1.000e-05, It/sec 0.308, Tokens/sec 228.484, Trained Tokens 602966, Peak mem 20.961 GB\n",
      "Iter 690: Train loss 1.893, Learning Rate 1.000e-05, It/sec 0.237, Tokens/sec 248.094, Trained Tokens 613452, Peak mem 20.961 GB\n",
      "Iter 700: Train loss 1.818, Learning Rate 1.000e-05, It/sec 0.337, Tokens/sec 268.671, Trained Tokens 621434, Peak mem 20.961 GB\n",
      "Iter 700: Saved adapter weights to adapters/adapters.safetensors and adapters/0000700_adapters.safetensors.\n",
      "Iter 710: Train loss 1.893, Learning Rate 1.000e-05, It/sec 0.336, Tokens/sec 265.523, Trained Tokens 629337, Peak mem 20.961 GB\n",
      "Iter 720: Train loss 1.919, Learning Rate 1.000e-05, It/sec 0.306, Tokens/sec 275.555, Trained Tokens 638332, Peak mem 20.961 GB\n",
      "Iter 730: Train loss 1.878, Learning Rate 1.000e-05, It/sec 0.254, Tokens/sec 266.714, Trained Tokens 648837, Peak mem 20.961 GB\n",
      "Iter 740: Train loss 1.970, Learning Rate 1.000e-05, It/sec 0.344, Tokens/sec 271.543, Trained Tokens 656740, Peak mem 20.961 GB\n",
      "Iter 750: Train loss 1.817, Learning Rate 1.000e-05, It/sec 0.325, Tokens/sec 277.821, Trained Tokens 665279, Peak mem 20.961 GB\n",
      "Iter 760: Train loss 1.878, Learning Rate 1.000e-05, It/sec 0.305, Tokens/sec 268.987, Trained Tokens 674106, Peak mem 20.961 GB\n",
      "Iter 770: Train loss 1.975, Learning Rate 1.000e-05, It/sec 0.258, Tokens/sec 261.294, Trained Tokens 684216, Peak mem 20.961 GB\n",
      "Iter 780: Train loss 1.781, Learning Rate 1.000e-05, It/sec 0.297, Tokens/sec 272.112, Trained Tokens 693375, Peak mem 20.961 GB\n",
      "Iter 790: Train loss 1.723, Learning Rate 1.000e-05, It/sec 0.335, Tokens/sec 283.253, Trained Tokens 701831, Peak mem 20.961 GB\n",
      "Iter 800: Val loss 1.855, Val took 46.502s\n",
      "Iter 800: Train loss 2.066, Learning Rate 1.000e-05, It/sec 2.387, Tokens/sec 2544.994, Trained Tokens 712494, Peak mem 20.961 GB\n",
      "Iter 800: Saved adapter weights to adapters/adapters.safetensors and adapters/0000800_adapters.safetensors.\n",
      "Iter 810: Train loss 1.918, Learning Rate 1.000e-05, It/sec 0.316, Tokens/sec 271.352, Trained Tokens 721079, Peak mem 20.961 GB\n",
      "Iter 820: Train loss 1.755, Learning Rate 1.000e-05, It/sec 0.322, Tokens/sec 278.982, Trained Tokens 729739, Peak mem 20.961 GB\n",
      "Iter 830: Train loss 1.860, Learning Rate 1.000e-05, It/sec 0.339, Tokens/sec 281.184, Trained Tokens 738033, Peak mem 20.961 GB\n",
      "Iter 840: Train loss 1.900, Learning Rate 1.000e-05, It/sec 0.257, Tokens/sec 281.568, Trained Tokens 749001, Peak mem 20.961 GB\n",
      "Iter 850: Train loss 1.803, Learning Rate 1.000e-05, It/sec 0.357, Tokens/sec 290.288, Trained Tokens 757122, Peak mem 20.961 GB\n",
      "Iter 860: Train loss 1.914, Learning Rate 1.000e-05, It/sec 0.358, Tokens/sec 277.365, Trained Tokens 764877, Peak mem 20.961 GB\n",
      "Iter 870: Train loss 1.867, Learning Rate 1.000e-05, It/sec 0.278, Tokens/sec 281.429, Trained Tokens 775009, Peak mem 20.961 GB\n",
      "Iter 880: Train loss 1.847, Learning Rate 1.000e-05, It/sec 0.264, Tokens/sec 281.130, Trained Tokens 785658, Peak mem 20.961 GB\n",
      "Iter 890: Train loss 1.823, Learning Rate 1.000e-05, It/sec 0.337, Tokens/sec 289.320, Trained Tokens 794235, Peak mem 20.961 GB\n",
      "Iter 900: Train loss 2.037, Learning Rate 1.000e-05, It/sec 0.321, Tokens/sec 286.497, Trained Tokens 803159, Peak mem 20.961 GB\n",
      "Iter 900: Saved adapter weights to adapters/adapters.safetensors and adapters/0000900_adapters.safetensors.\n",
      "Iter 910: Train loss 1.867, Learning Rate 1.000e-05, It/sec 0.329, Tokens/sec 286.399, Trained Tokens 811858, Peak mem 20.961 GB\n",
      "Iter 920: Train loss 1.776, Learning Rate 1.000e-05, It/sec 0.293, Tokens/sec 280.509, Trained Tokens 821418, Peak mem 20.961 GB\n",
      "Iter 930: Train loss 1.802, Learning Rate 1.000e-05, It/sec 0.302, Tokens/sec 281.773, Trained Tokens 830750, Peak mem 20.961 GB\n",
      "Iter 940: Train loss 1.838, Learning Rate 1.000e-05, It/sec 0.259, Tokens/sec 277.968, Trained Tokens 841477, Peak mem 20.961 GB\n",
      "Iter 950: Train loss 1.834, Learning Rate 1.000e-05, It/sec 0.326, Tokens/sec 282.227, Trained Tokens 850123, Peak mem 20.961 GB\n",
      "Iter 960: Train loss 1.785, Learning Rate 1.000e-05, It/sec 0.266, Tokens/sec 284.188, Trained Tokens 860787, Peak mem 20.961 GB\n",
      "Iter 970: Train loss 1.892, Learning Rate 1.000e-05, It/sec 0.289, Tokens/sec 277.961, Trained Tokens 870403, Peak mem 20.961 GB\n",
      "Iter 980: Train loss 1.837, Learning Rate 1.000e-05, It/sec 0.266, Tokens/sec 285.800, Trained Tokens 881146, Peak mem 20.961 GB\n",
      "Iter 990: Train loss 1.956, Learning Rate 1.000e-05, It/sec 0.416, Tokens/sec 288.520, Trained Tokens 888079, Peak mem 20.961 GB\n",
      "Iter 1000: Val loss 1.777, Val took 43.014s\n",
      "Iter 1000: Train loss 1.908, Learning Rate 1.000e-05, It/sec 4.052, Tokens/sec 3599.607, Trained Tokens 896962, Peak mem 20.961 GB\n",
      "Iter 1000: Saved adapter weights to adapters/adapters.safetensors and adapters/0001000_adapters.safetensors.\n",
      "Saved final adapter weights to adapters/adapters.safetensors.\n",
      "Testing\n",
      "Test loss 1.859, Test ppl 6.414.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "%%bash\n",
    "python -m mlx_lm.lora --model models_mlx/ --data . --test --train --iters 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prompt</th>\n",
       "      <th>label</th>\n",
       "      <th>completion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Seit ca. 6 Jahren Shcmerzen re. Kniegelenk; Be...</td>\n",
       "      <td>other</td>\n",
       "      <td>Aktivierte Gonarthrose rechts</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2019 erstmalig nach Ergometertrainiung Schmerz...</td>\n",
       "      <td>other</td>\n",
       "      <td>Krankheit der Patella; nicht näher bezeichnet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Damals Judo und Kickboxen gemacht; kein größer...</td>\n",
       "      <td>other</td>\n",
       "      <td>Chondromalacia patellae</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Vor 15 Jahren erstmalige Patellaluxation links...</td>\n",
       "      <td>other</td>\n",
       "      <td>Habituelle Luxation der Patella</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>z.n Patellaluxation 1992; seit vielen Jahren K...</td>\n",
       "      <td>other</td>\n",
       "      <td>funktionelles Patellaspitzensyndrom links</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              prompt  label  \\\n",
       "0  Seit ca. 6 Jahren Shcmerzen re. Kniegelenk; Be...  other   \n",
       "1  2019 erstmalig nach Ergometertrainiung Schmerz...  other   \n",
       "2  Damals Judo und Kickboxen gemacht; kein größer...  other   \n",
       "3  Vor 15 Jahren erstmalige Patellaluxation links...  other   \n",
       "4  z.n Patellaluxation 1992; seit vielen Jahren K...  other   \n",
       "\n",
       "                                      completion  \n",
       "0                  Aktivierte Gonarthrose rechts  \n",
       "1  Krankheit der Patella; nicht näher bezeichnet  \n",
       "2                        Chondromalacia patellae  \n",
       "3                Habituelle Luxation der Patella  \n",
       "4      funktionelles Patellaspitzensyndrom links  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"test.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = df[\"prompt\"].iloc[2]\n",
    "completion = df[\"completion\"].iloc[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Damals Judo und Kickboxen gemacht; kein größeres Trauma erinnerlich. Seit 2 Jahren immer wieder mal Schmerzen im rechtem Knie bei langer Belastung (ab 10 km Joggen oder 30 km Radfahren). Konservative Behandlung (6x Physiotherapie) ohne Erfolg. In letzter Zeit keine Schmerzprogredienz. Wünscht Zweitmeinung. Beinachse: 3QF varisch             Gangbild: flüssig re.Knie: keine Rötung; keine Überwärmung; deutliche Schwellung über med GS mit tastbarer Fluktation; 0/0/140; klinisch kein i.a. Erguss; Seitenbänder fest; Lachman neg. fest; Pivot shift neg.; hintere Schublade ++ unsicher; med step off deutlich vermindert; kein DS med GS; Steiman I pos; Payr pos. Instrumentelle Stabilitätsmessung:  re.   9  mm; li.9   mm Ap Translation in 90°:    re.  19  mm; li.10   mm 43.0 male\n",
      "Chondromalacia patellae\n"
     ]
    }
   ],
   "source": [
    "print(prompt)\n",
    "print(completion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========\n",
      "Prompt: <|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "2019 erstmalig nach Ergometertrainiung Schmerzen; KnochenDamals Judo und Kickboxen gemacht; kein größeres Trauma erinnerlich. Seit 2 Jahren immer wieder mal Schmerzen im rechtem Knie bei langer Belastung (ab 10 km Joggen oder 30 km Radfahren). Konservative Behandlung (6x Physiotherapie) ohne Erfolg. In letzter Zeit keine Schmerzprogredienz. Wünscht Zweitmeinung. Beinachse: 3QF varisch             Gangbild: flüssig re.Knie: keine Rötung; keine Überwärmung; deutliche Schwellung über med GS mit tastbarer Fluktation; 0/0/140; klinisch kein i.a. Erguss; Seitenbänder fest; Lachman neg. fest; Pivot shift neg.; hintere Schublade ++ unsicher; med step off deutlich vermindert; kein DS med GS; Steiman I pos; Payr pos. Instrumentelle Stabilitätsmessung:  re.   9  mm; li.9   mm Ap Translation in 90°:    re.  19  mm; li.10   mm 43.0 male<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "\n",
      "Sonstige Meniskusschädigungen: Hinterhorn des Innenmeniskus\n",
      "==========\n",
      "Prompt: 175.062 tokens-per-sec\n",
      "Generation: 15.816 tokens-per-sec\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "python -m mlx_lm.generate --model models_mlx \\\n",
    "                --max-tokens 120 \\\n",
    "               --adapter-path adapters \\\n",
    "               --prompt \"2019 erstmalig nach Ergometertrainiung Schmerzen; KnochenDamals Judo und Kickboxen gemacht; kein größeres Trauma erinnerlich. Seit 2 Jahren immer wieder mal Schmerzen im rechtem Knie bei langer Belastung (ab 10 km Joggen oder 30 km Radfahren). Konservative Behandlung (6x Physiotherapie) ohne Erfolg. In letzter Zeit keine Schmerzprogredienz. Wünscht Zweitmeinung. Beinachse: 3QF varisch             Gangbild: flüssig re.Knie: keine Rötung; keine Überwärmung; deutliche Schwellung über med GS mit tastbarer Fluktation; 0/0/140; klinisch kein i.a. Erguss; Seitenbänder fest; Lachman neg. fest; Pivot shift neg.; hintere Schublade ++ unsicher; med step off deutlich vermindert; kein DS med GS; Steiman I pos; Payr pos. Instrumentelle Stabilitätsmessung:  re.   9  mm; li.9   mm Ap Translation in 90°:    re.  19  mm; li.10   mm 43.0 male\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test data validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading datasets\n",
      "Testing\n",
      "Test loss 1.859, Test ppl 6.414.\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "python -m mlx_lm.lora \\\n",
    "    --model models_mlx \\\n",
    "    --adapter-path adapters \\\n",
    "    --data . \\\n",
    "    --test"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
