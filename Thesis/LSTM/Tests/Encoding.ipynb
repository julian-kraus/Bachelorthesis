{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-06-19T16:11:26.754369Z",
     "start_time": "2024-06-19T16:11:25.327519Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Bidirectional, BatchNormalization\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import IO\n",
    "from keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/juliankraus/Library/CloudStorage/OneDrive-Personal/Uni/Languages/Code/pythonProject/.venv/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-19T16:34:05.836335Z",
     "start_time": "2024-06-19T16:34:05.742055Z"
    }
   },
   "cell_type": "code",
   "source": [
    "df = IO.load_excel(\"./data/generated_examinations.xlsx\").dropna()\n",
    "category_counts = df['Diagnose'].value_counts()\n",
    "# Step 2: Determine the count of the least frequent category\n",
    "min_count = category_counts.min()\n",
    "# Step 3: Sample the same number of entries for each category\n",
    "data = pd.concat([\n",
    "    df[df['Diagnose'] == category].sample(min_count, random_state=1)\n",
    "    for category in category_counts.index\n",
    "])\n",
    "# Step 4: Reset index (optional)\n",
    "data.reset_index(drop=True, inplace=True)\n",
    "category_counts = data['Diagnose'].value_counts()\n",
    "\n",
    "# Print the amount of values in each category\n",
    "print(\"Category counts in 'Diagnosis' column:\")\n",
    "for category, count in category_counts.items():\n",
    "    print(f\"{category}: {count}\")\n",
    "data.head()"
   ],
   "id": "74c38da3540b8254",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Category counts in 'Diagnosis' column:\n",
      "Kreuzband: 173\n",
      "Meniskus: 173\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "   Patientid_x                                           Text_Bef  \\\n",
       "0       307174  Beinachse gerade Rechtes Kniegelenk: Extension...   \n",
       "1       304155  Gangbild unter Vollbelastung flüssig,  rechtes...   \n",
       "2       405299  Gangbild unter Vollbelastung flüssig und mit f...   \n",
       "3       307079  Gangbild unter Vollbelastung mit fehlender Sch...   \n",
       "4       306656  Gangbild flüssig, Viertelkniebeuge im Einbeins...   \n",
       "\n",
       "                                            Text_ANA Geschlecht   Diagnose  \\\n",
       "0  OP am 27.05.2024. Olsi Reka stellt sich heute ...   männlich  Kreuzband   \n",
       "1  2 Mon nach AC Knie und MFX lat Tibiaplateau , ...   männlich  Kreuzband   \n",
       "2  2021 Arbeitsunfall mit Syndesmosenriss rechts....   weiblich  Kreuzband   \n",
       "3  Schmerz und Entzündungsgefühl im Knie seit der...   männlich  Kreuzband   \n",
       "4  WV bei LCA Partialinsuffizienz. Aktuell komme ...   männlich  Kreuzband   \n",
       "\n",
       "                                                 All  \\\n",
       "0  beinachse gerade rechtes kniegelenk extension/...   \n",
       "1  gangbild unter vollbelastung flüssig, rechtes ...   \n",
       "2  gangbild unter vollbelastung flüssig, mit fehl...   \n",
       "3  gangbild unter vollbelastung mit fehlender sch...   \n",
       "4  gangbild flüssig, viertelkniebeuge im einbeins...   \n",
       "\n",
       "                                           Formatted  \\\n",
       "0  beinachse gerade rechtes kniegelenk extension/...   \n",
       "1  gangbild unter vollbelastung flüssig, rechtes ...   \n",
       "2  gangbild unter vollbelastung flüssig, rechtes ...   \n",
       "3  gangbild unter vollbelastung mit fehlender sch...   \n",
       "4           gangbild flüssig, knie links erguss kein   \n",
       "\n",
       "                                      generated_text  \n",
       "0   Das rechte Kniegelenk hat keine Überstreckung...  \n",
       "1   Das Gangbild ist unter voller Belastung flüss...  \n",
       "2   Das Gangbild unter voller Belastung ist flüss...  \n",
       "3   Unter voller Belastung gibt es ein Gangbild m...  \n",
       "4   Das Gangbild ist flüssig, und es gibt keinen ...  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Patientid_x</th>\n",
       "      <th>Text_Bef</th>\n",
       "      <th>Text_ANA</th>\n",
       "      <th>Geschlecht</th>\n",
       "      <th>Diagnose</th>\n",
       "      <th>All</th>\n",
       "      <th>Formatted</th>\n",
       "      <th>generated_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>307174</td>\n",
       "      <td>Beinachse gerade Rechtes Kniegelenk: Extension...</td>\n",
       "      <td>OP am 27.05.2024. Olsi Reka stellt sich heute ...</td>\n",
       "      <td>männlich</td>\n",
       "      <td>Kreuzband</td>\n",
       "      <td>beinachse gerade rechtes kniegelenk extension/...</td>\n",
       "      <td>beinachse gerade rechtes kniegelenk extension/...</td>\n",
       "      <td>Das rechte Kniegelenk hat keine Überstreckung...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>304155</td>\n",
       "      <td>Gangbild unter Vollbelastung flüssig,  rechtes...</td>\n",
       "      <td>2 Mon nach AC Knie und MFX lat Tibiaplateau , ...</td>\n",
       "      <td>männlich</td>\n",
       "      <td>Kreuzband</td>\n",
       "      <td>gangbild unter vollbelastung flüssig, rechtes ...</td>\n",
       "      <td>gangbild unter vollbelastung flüssig, rechtes ...</td>\n",
       "      <td>Das Gangbild ist unter voller Belastung flüss...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>405299</td>\n",
       "      <td>Gangbild unter Vollbelastung flüssig und mit f...</td>\n",
       "      <td>2021 Arbeitsunfall mit Syndesmosenriss rechts....</td>\n",
       "      <td>weiblich</td>\n",
       "      <td>Kreuzband</td>\n",
       "      <td>gangbild unter vollbelastung flüssig, mit fehl...</td>\n",
       "      <td>gangbild unter vollbelastung flüssig, rechtes ...</td>\n",
       "      <td>Das Gangbild unter voller Belastung ist flüss...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>307079</td>\n",
       "      <td>Gangbild unter Vollbelastung mit fehlender Sch...</td>\n",
       "      <td>Schmerz und Entzündungsgefühl im Knie seit der...</td>\n",
       "      <td>männlich</td>\n",
       "      <td>Kreuzband</td>\n",
       "      <td>gangbild unter vollbelastung mit fehlender sch...</td>\n",
       "      <td>gangbild unter vollbelastung mit fehlender sch...</td>\n",
       "      <td>Unter voller Belastung gibt es ein Gangbild m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>306656</td>\n",
       "      <td>Gangbild flüssig, Viertelkniebeuge im Einbeins...</td>\n",
       "      <td>WV bei LCA Partialinsuffizienz. Aktuell komme ...</td>\n",
       "      <td>männlich</td>\n",
       "      <td>Kreuzband</td>\n",
       "      <td>gangbild flüssig, viertelkniebeuge im einbeins...</td>\n",
       "      <td>gangbild flüssig, knie links erguss kein</td>\n",
       "      <td>Das Gangbild ist flüssig, und es gibt keinen ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-19T16:36:13.151064Z",
     "start_time": "2024-06-19T16:36:13.111013Z"
    }
   },
   "cell_type": "code",
   "source": [
    "data['FullText'] = data['Text_ANA'] + ' ' + data['generated_text']\n",
    "# Encode labels\n",
    "label_encoder = LabelEncoder()\n",
    "data['label'] = label_encoder.fit_transform(data['Diagnose'])\n",
    "num_classes = (len(data['label'].unique()))\n",
    "print(f\"Number of classes: {num_classes}\")\n",
    "# Split the dataset into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(data['FullText'], data['label'], test_size=0.2, random_state=42)\n",
    "train_labels = np.unique(y_train)\n",
    "test_labels = np.unique(y_test)\n",
    "\n",
    "# Tokenize the text\n",
    "tokenizer = Tokenizer(num_words=10000)\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "X_train_seq = tokenizer.texts_to_sequences(X_train)\n",
    "X_test_seq = tokenizer.texts_to_sequences(X_test)\n",
    "print(X_train_seq[0])\n",
    "\n",
    "\n",
    "# Find the maximum sequence length\n",
    "max_length_train = max(len(seq) for seq in X_train_seq)\n",
    "max_length_test = max(len(seq) for seq in X_test_seq)\n",
    "actual_max_length = max(max_length_train, max_length_test)\n",
    "print(f\"Actual maximum length: {actual_max_length}\")\n",
    "\n",
    "# Pad the sequences\n",
    "max_length = 100\n",
    "X_train_pad = pad_sequences(X_train_seq, maxlen=max_length, padding='post')\n",
    "X_test_pad = pad_sequences(X_test_seq, maxlen=max_length, padding='post')\n",
    "print(X_train_pad[0])\n",
    "\n",
    "# Convert labels to categorical\n",
    "y_train_cat = to_categorical(y_train)\n",
    "print(y_train_cat)\n",
    "y_test_cat = to_categorical(y_test)\n",
    "IO.save_training_data(\"MenKreuzAnEx\", X_train, X_test, y_train, y_test, X_train_pad, X_test_pad, y_train_cat, y_test_cat, label_encoder.classes_)\n"
   ],
   "id": "1acd05708174e383",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of classes: 2\n",
      "[61, 101, 25, 33, 109, 109, 43, 81, 6, 20, 189, 44, 293, 48, 56, 60, 5, 478, 18, 224, 687, 375, 224, 2, 26, 31, 13, 37, 82, 10, 29, 1, 4, 12, 14, 1, 5, 23, 9, 2, 6, 30, 52, 1, 4, 24, 45, 35, 3, 27, 18, 62, 78]\n",
      "Actual maximum length: 112\n",
      "[ 61 101  25  33 109 109  43  81   6  20 189  44 293  48  56  60   5 478\n",
      "  18 224 687 375 224   2  26  31  13  37  82  10  29   1   4  12  14   1\n",
      "   5  23   9   2   6  30  52   1   4  24  45  35   3  27  18  62  78   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0]\n",
      "[[1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]]\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "e04b3ac5638ed9d2"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
