{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Implementation",
   "id": "7f1a9eb39acea83c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Embedding, LSTM, Dense, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "# Ensure TensorFlow is using the Metal backend\n",
    "print(tf.config.list_physical_devices('GPU'))\n",
    "# Tokenize the texts\n",
    "vocab_size = 10000  # Size of the vocabulary\n",
    "max_length = 400  # Length of input sequences\n",
    "\n",
    "tokenizer = Tokenizer(num_words=vocab_size)\n",
    "tokenizer.fit_on_texts(texts)\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "padded_sequences = pad_sequences(sequences, maxlen=max_length, padding='post')\n",
    "\n",
    "# Define model parameters\n",
    "embedding_dim = 300  # Dimension of the embedding vectors\n",
    "lstm_units = 128  # Number of LSTM units\n",
    "num_classes = 2  # Number of output classes (for binary classification)\n",
    "\n",
    "# Define the input layer\n",
    "input_text = Input(shape=(max_length,), dtype='int32', name='text_input')\n",
    "\n",
    "# Embedding layer\n",
    "embedding = Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=max_length)(input_text)\n",
    "\n",
    "# Two LSTM layers\n",
    "x = LSTM(lstm_units, return_sequences=True)(embedding)\n",
    "x = Dropout(0.5)(x)\n",
    "x = LSTM(lstm_units)(x)\n",
    "x = Dropout(0.5)(x)\n",
    "\n",
    "# Output layer\n",
    "output = Dense(num_classes, activation='softmax')(x)\n",
    "\n",
    "# Define the model\n",
    "model = Model(inputs=input_text, outputs=output)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Summary of the model\n",
    "model.summary()"
   ],
   "id": "240184416e153ac3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Train the model\n",
    "model.fit(padded_sequences, labels, epochs=10, batch_size=32, validation_split=0.2)"
   ],
   "id": "699d72a5d33e06fc"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Embeddings",
   "id": "d1eef0a01124d6dd"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Word2Vec",
   "id": "49b8701a82ac6ebd"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import spacy\n",
    "from gensim.models import KeyedVectors\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import Input, Embedding, LSTM, Dense, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "# Load spaCy model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Remove non-alphabetic characters\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    doc = nlp(text)\n",
    "    stop_words = spacy.lang.en.stop_words.STOP_WORDS\n",
    "    words = [token.text for token in doc if token.text.lower() not in stop_words and token.is_alpha]\n",
    "    return words\n",
    "\n",
    "def get_word2vec_embeddings(text, model, vector_size=300):\n",
    "    tokens = preprocess_text(text)\n",
    "    embeddings = np.zeros((len(tokens), vector_size))\n",
    "    for i, token in enumerate(tokens):\n",
    "        if token in model.wv:\n",
    "            embeddings[i] = model.wv[token]\n",
    "        else:\n",
    "            embeddings[i] = np.zeros(vector_size)\n",
    "    return embeddings\n",
    "\n",
    "# Load pre-trained Word2Vec embeddings\n",
    "word2vec_path = 'path/to/word2vec.bin'  # Replace with your Word2Vec file path\n",
    "model = KeyedVectors.load_word2vec_format(word2vec_path, binary=True)\n",
    "\n",
    "# Convert texts to embeddings\n",
    "word2vec_embeddings = [get_word2vec_embeddings(text, model) for text in texts]\n",
    "padded_embeddings = pad_sequences(word2vec_embeddings, padding='post', dtype='float32')\n",
    "\n",
    "# Determine the max length of the sequences\n",
    "max_length = padded_embeddings.shape[1]\n",
    "\n",
    "# Define model parameters\n",
    "embedding_dim = 300  # Dimension of the Word2Vec embeddings\n",
    "lstm_units = 128  # Number of LSTM units\n",
    "num_classes = 2  # Number of output classes (for binary classification)\n",
    "\n",
    "# Define the input layer\n",
    "input_text = Input(shape=(max_length, embedding_dim), dtype='float32', name='text_input')\n",
    "\n",
    "# Two LSTM layers\n",
    "x = LSTM(lstm_units, return_sequences=True)(input_text)\n",
    "x = Dropout(0.5)(x)\n",
    "x = LSTM(lstm_units)(x)\n",
    "x = Dropout(0.5)(x)\n",
    "\n",
    "# Output layer\n",
    "output = Dense(num_classes, activation='softmax')(x)\n",
    "\n",
    "# Define the model\n",
    "model_word2vec = Model(inputs=input_text, outputs=output)\n",
    "\n",
    "# Compile the model\n",
    "model_word2vec.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Summary of the model\n",
    "model_word2vec.summary()\n",
    "\n",
    "# Train the model\n",
    "model_word2vec.fit(padded_embeddings, labels, epochs=10, batch_size=32, validation_split=0.2)\n"
   ],
   "id": "c32a761593ece509"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Bert",
   "id": "7b5eeac98007cfd3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, LSTM, Dense, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "from transformers import BertTokenizer, TFBertModel\n",
    "\n",
    "# Function to generate word embeddings for a given text\n",
    "def generate_bert_embeddings(text, tokenizer, model):\n",
    "    inputs = tokenizer(text, return_tensors=\"tf\", truncation=True, padding=True)\n",
    "    outputs = model(inputs)\n",
    "    embeddings = outputs.last_hidden_state\n",
    "    return embeddings\n",
    "\n",
    "model_name = \"GerMedBERT/medbert-512\"\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "model = TFBertModel.from_pretrained(model_name)\n",
    "\n",
    "bert_embeddings = [generate_bert_embeddings(text, tokenizer, model).numpy().squeeze(0) for text in texts]\n",
    "padded_embeddings = pad_sequences(bert_embeddings, padding='post', dtype='float32')\n",
    "\n",
    "\n",
    "# Define model parameters\n",
    "lstm_units = 128  # Number of LSTM units\n",
    "num_classes = 2  # Number of output classes (for binary classification)\n",
    "embedding_dim = padded_embeddings.shape[-1]\n",
    "\n",
    "# Define the input layers\n",
    "input_text = Input(shape=(padded_embeddings.shape[1], embedding_dim), dtype='float32', name='text_input')\n",
    "\n",
    "# Two LSTM layers\n",
    "x = LSTM(lstm_units, return_sequences=True)(bert_embeddings)\n",
    "x = Dropout(0.5)(x)\n",
    "x = LSTM(lstm_units)(x)\n",
    "x = Dropout(0.5)(x)\n",
    "\n",
    "# Output layer\n",
    "output = Dense(num_classes, activation='softmax')(x)\n",
    "\n",
    "# Define the model\n",
    "model_bert = Model(inputs=input_text, outputs=output)\n",
    "\n",
    "# Compile the model\n",
    "model_bert.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Summary of the model\n",
    "model_bert.summary()"
   ],
   "id": "590d28cb82a9c8f4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "model_bert.fit(padded_embeddings, labels, epochs=10, batch_size=32, validation_split=0.2)",
   "id": "c994d8a30426bf0e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Batchnormalization",
   "id": "b9ee737834ef8042"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### On input",
   "id": "bb7a3375df2cedee"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
