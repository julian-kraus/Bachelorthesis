{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-07-22T09:55:28.274305Z",
     "start_time": "2024-07-22T09:55:25.179736Z"
    }
   },
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.layers import Input, Embedding, LSTM, Dense, Dropout, Bidirectional, Conv1D, GlobalMaxPooling1D, Concatenate, AdditiveAttention\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.layers import AdditiveAttention\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import tensorflow as tf\n",
    "from utils import eval, eval_training, get_train_test_data\n",
    "import re\n",
    "import numpy as np\n",
    "import spacy\n",
    "from gensim.models import Word2Vec\n",
    "from transformers import BertTokenizer, TFBertModel\n",
    "\n",
    "\n",
    "os.environ['TF_NUM_INTEROP_THREADS'] = '10'\n",
    "os.environ['TF_NUM_INTRAOP_THREADS'] = '10'\n",
    "\n",
    "# Configure TensorFlow session for multi-threading\n",
    "tf.config.threading.set_inter_op_parallelism_threads(10)\n",
    "tf.config.threading.set_intra_op_parallelism_threads(10)\n",
    "# Ensure TensorFlow is using the Metal backend\n",
    "gpu = len (tf.config.list_physical_devices ('GPU'))>0\n",
    "print (\"GPU is\", \"available\" if gpu else \"NOT AVAILABLE\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU is available\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-22T09:57:14.977750Z",
     "start_time": "2024-07-22T09:57:14.524306Z"
    }
   },
   "cell_type": "code",
   "source": [
    "data_label = \"data\"\n",
    "predict_label = \"label\"\n",
    "balanced = False\n",
    "labels = [\"meniskus_urgent\", \"cruciate_urgent\"]\n",
    "class_weights = False\n",
    "sample_weights = False\n",
    "df = pd.read_csv('../data/balanced_classification_dataset.csv')\n",
    "\n",
    "df = df.sample(frac=1).reset_index(drop=True)\n",
    "if labels is not None:\n",
    "  df = df[df['label'].isin(labels)]\n",
    "\n",
    "# Split data into training and temporary data (the remaining 40%)\n",
    "train_df, temp_df = train_test_split(df, test_size=0.4, random_state=42, stratify=df[predict_label])\n",
    "\n",
    "# Split the temporary data into validation and test sets (each 50% of temporary, thus 20% of total each)\n",
    "valid_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42, stratify=temp_df[predict_label])\n",
    "\n",
    "# Check the size of each set\n",
    "print(\"Training set size:\", len(train_df))\n",
    "print(\"Validation set size:\", len(valid_df))\n",
    "print(\"Test set size:\", len(test_df))\n",
    "# Prepare data for training\n",
    "train_texts = train_df[data_label]\n",
    "valid_texts = valid_df[data_label]\n",
    "test_texts = test_df[data_label]\n",
    "\n",
    "# Initialize the label encoder\n",
    "all_labels = pd.concat([train_df[predict_label], valid_df[predict_label], test_df[predict_label]])\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(all_labels)\n",
    "# Fit label encoder and return encoded labels as integers\n",
    "train_labels_enc = label_encoder.transform(train_df[predict_label])\n",
    "valid_labels_enc = label_encoder.transform(valid_df[predict_label])\n",
    "test_labels_enc = label_encoder.transform(test_df[predict_label])\n",
    "\n",
    "# Convert labels to categorical one-hot encoding\n",
    "train_labels = to_categorical(train_labels_enc)\n",
    "valid_labels = to_categorical(valid_labels_enc)\n",
    "test_labels = to_categorical(test_labels_enc)\n",
    "\n",
    "num_classes = len(train_df[predict_label].unique())"
   ],
   "id": "bbce78941e041915",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 14946\n",
      "Validation set size: 4982\n",
      "Test set size: 4982\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-22T09:57:16.763054Z",
     "start_time": "2024-07-22T09:57:16.761318Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Define model parameters\n",
    "embedding_dim = 300  # Dimension of the embedding vectors\n",
    "lstm_units = 64\n",
    "epochs = 10\n",
    "batch_size = 64"
   ],
   "id": "534ec3e56f51a347",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# No Embedding Model",
   "id": "1b4acdd9fdcece87"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-22T09:48:56.178787Z",
     "start_time": "2024-07-22T09:48:56.099586Z"
    }
   },
   "cell_type": "code",
   "source": [
    "optimizer = Adam()\n",
    "loss = tf.keras.losses.CategoricalCrossentropy()\n",
    "\n",
    "metrics=[\n",
    "      tf.keras.metrics.CategoricalAccuracy(name='accuracy'),\n",
    "      tf.keras.metrics.AUC(name='auc'),\n",
    "      tf.keras.metrics.F1Score(name='f1_score'),\n",
    "]"
   ],
   "id": "56b45e19f8107774",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-22 11:48:56.102126: I metal_plugin/src/device/metal_device.cc:1154] Metal device set to: Apple M3 Max\n",
      "2024-07-22 11:48:56.102153: I metal_plugin/src/device/metal_device.cc:296] systemMemory: 64.00 GB\n",
      "2024-07-22 11:48:56.102157: I metal_plugin/src/device/metal_device.cc:313] maxCacheSize: 24.00 GB\n",
      "2024-07-22 11:48:56.102170: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2024-07-22 11:48:56.102184: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-22T09:48:59.385506Z",
     "start_time": "2024-07-22T09:48:56.796673Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Tokenize texts\n",
    "tokenizer = Tokenizer(num_words=10000)\n",
    "tokenizer.fit_on_texts(train_texts)  # Only fit on train data\n",
    "word_index = tokenizer.word_index\n",
    "train_sequences = tokenizer.texts_to_sequences(train_texts)\n",
    "valid_sequences = tokenizer.texts_to_sequences(valid_texts)\n",
    "test_sequences = tokenizer.texts_to_sequences(test_texts)\n",
    "max_length = max(len(sequence) for sequence in train_sequences) + 30\n",
    "print(max_length)\n",
    "# Padding sequences\n",
    "train_padded = pad_sequences(train_sequences, maxlen=max_length, padding='post')\n",
    "valid_padded = pad_sequences(valid_sequences, maxlen=max_length, padding='post')\n",
    "test_padded = pad_sequences(test_sequences, maxlen=max_length, padding='post')"
   ],
   "id": "f74efd41ed49c687",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "363\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-21T15:01:30.193904Z",
     "start_time": "2024-07-21T15:01:29.662480Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Define the input layer\n",
    "input_text = Input(shape=(max_length,), dtype='int32', name='text_input')\n",
    "\n",
    "# Embedding layer\n",
    "embedding = Embedding(input_dim=len(word_index), output_dim=embedding_dim)(input_text)\n",
    "\n",
    "# Two LSTM layers\n",
    "x = Bidirectional(LSTM(units=lstm_units, return_sequences=True))(embedding)\n",
    "x = Dropout(0.5)(x)\n",
    "x = Bidirectional(LSTM(units=lstm_units))(embedding)\n",
    "\n",
    "\n",
    "# Output layer\n",
    "output = Dense(num_classes, activation='softmax')(x)\n",
    "\n",
    "# Define the model\n",
    "model_base = Model(inputs=input_text, outputs=output)\n",
    "\n",
    "# Compile the model\n",
    "model_base.compile(optimizer=optimizer, loss=loss, metrics=metrics)"
   ],
   "id": "1e1b201b9f9a4afa",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-21T15:09:18.439972Z",
     "start_time": "2024-07-21T15:01:30.194550Z"
    }
   },
   "cell_type": "code",
   "source": [
    "history_base = model_base.fit(train_padded, train_labels, epochs=epochs, batch_size=batch_size,\n",
    "                    validation_data=(valid_padded, valid_labels))"
   ],
   "id": "5e3d9836778cbb9c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-21 17:01:30.511902: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:117] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[1m425/425\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m53s\u001B[0m 114ms/step - accuracy: 0.4673 - auc: 0.8264 - f1_score: 0.2134 - loss: 1.2916 - val_accuracy: 0.5768 - val_auc: 0.8809 - val_f1_score: 0.3012 - val_loss: 1.0893\n",
      "Epoch 2/10\n",
      "\u001B[1m425/425\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m47s\u001B[0m 111ms/step - accuracy: 0.6093 - auc: 0.8894 - f1_score: 0.3176 - loss: 1.0492 - val_accuracy: 0.5828 - val_auc: 0.8856 - val_f1_score: 0.3058 - val_loss: 1.0687\n",
      "Epoch 3/10\n",
      "\u001B[1m425/425\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m46s\u001B[0m 109ms/step - accuracy: 0.6710 - auc: 0.9115 - f1_score: 0.3510 - loss: 0.9337 - val_accuracy: 0.5919 - val_auc: 0.8881 - val_f1_score: 0.3085 - val_loss: 1.0558\n",
      "Epoch 4/10\n",
      "\u001B[1m425/425\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m46s\u001B[0m 108ms/step - accuracy: 0.7102 - auc: 0.9277 - f1_score: 0.3827 - loss: 0.8423 - val_accuracy: 0.5911 - val_auc: 0.8866 - val_f1_score: 0.3268 - val_loss: 1.0754\n",
      "Epoch 5/10\n",
      "\u001B[1m425/425\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m46s\u001B[0m 108ms/step - accuracy: 0.7514 - auc: 0.9427 - f1_score: 0.4462 - loss: 0.7437 - val_accuracy: 0.5881 - val_auc: 0.8817 - val_f1_score: 0.3276 - val_loss: 1.1244\n",
      "Epoch 6/10\n",
      "\u001B[1m425/425\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m46s\u001B[0m 109ms/step - accuracy: 0.7938 - auc: 0.9560 - f1_score: 0.4977 - loss: 0.6432 - val_accuracy: 0.5810 - val_auc: 0.8748 - val_f1_score: 0.3324 - val_loss: 1.1875\n",
      "Epoch 7/10\n",
      "\u001B[1m425/425\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m47s\u001B[0m 110ms/step - accuracy: 0.8135 - auc: 0.9642 - f1_score: 0.5477 - loss: 0.5779 - val_accuracy: 0.5794 - val_auc: 0.8675 - val_f1_score: 0.3405 - val_loss: 1.2584\n",
      "Epoch 8/10\n",
      "\u001B[1m425/425\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m46s\u001B[0m 107ms/step - accuracy: 0.8314 - auc: 0.9708 - f1_score: 0.5930 - loss: 0.5207 - val_accuracy: 0.5814 - val_auc: 0.8650 - val_f1_score: 0.3581 - val_loss: 1.3154\n",
      "Epoch 9/10\n",
      "\u001B[1m425/425\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m45s\u001B[0m 107ms/step - accuracy: 0.8589 - auc: 0.9774 - f1_score: 0.6515 - loss: 0.4532 - val_accuracy: 0.5688 - val_auc: 0.8581 - val_f1_score: 0.3518 - val_loss: 1.3908\n",
      "Epoch 10/10\n",
      "\u001B[1m425/425\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m46s\u001B[0m 107ms/step - accuracy: 0.8678 - auc: 0.9807 - f1_score: 0.6777 - loss: 0.4178 - val_accuracy: 0.5635 - val_auc: 0.8501 - val_f1_score: 0.3564 - val_loss: 1.4569\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Word2Vec Embedding Model",
   "id": "4b76817940e82536"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-21T18:53:31.667503Z",
     "start_time": "2024-07-21T18:53:31.653540Z"
    }
   },
   "cell_type": "code",
   "source": [
    "optimizer = Adam()\n",
    "loss = tf.keras.losses.CategoricalCrossentropy()\n",
    "embedding_dim = 300\n",
    "metrics=[\n",
    "      tf.keras.metrics.CategoricalAccuracy(name='accuracy'),\n",
    "      tf.keras.metrics.AUC(name='auc'),\n",
    "      tf.keras.metrics.F1Score(name='f1_score'),\n",
    "]"
   ],
   "id": "630e5d048c59a116",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-21T18:53:26.754976Z",
     "start_time": "2024-07-21T18:53:17.964138Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Load spaCy model\n",
    "nlp = spacy.load('de_core_news_sm')\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Remove non-alphabetic characters\n",
    "    text = re.sub(r'[^a-zA-ZäöüÄÖÜß\\s]', '', text)\n",
    "    doc = nlp(text)\n",
    "    stop_words = spacy.lang.de.stop_words.STOP_WORDS\n",
    "    words = [token.text for token in doc if token.text.lower() not in stop_words and token.is_alpha]\n",
    "    return words\n",
    "\n",
    "def get_word2vec_embeddings(text, model, vector_size=300):\n",
    "    tokens = preprocess_text(text)\n",
    "    embeddings = np.zeros((len(tokens), vector_size))\n",
    "    for i, token in enumerate(tokens):\n",
    "        if token in model.wv:\n",
    "            embeddings[i] = model.wv[token]\n",
    "        else:\n",
    "            embeddings[i] = np.zeros(vector_size)\n",
    "    return embeddings\n",
    "\n",
    "# Load pre-trained Word2Vec embeddings\n",
    "w2v = Word2Vec.load(\"../data/word2vec.model\")"
   ],
   "id": "c86a280b2a154a02",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-21T19:02:29.784288Z",
     "start_time": "2024-07-21T18:53:39.082631Z"
    }
   },
   "cell_type": "code",
   "source": [
    "w2v_train_embeddings = [get_word2vec_embeddings(text, w2v) for text in train_texts]\n",
    "w2v_valid_embeddings = [get_word2vec_embeddings(text, w2v) for text in valid_texts]\n",
    "w2v_test_embeddings = [get_word2vec_embeddings(text, w2v) for text in test_texts]\n",
    "max_length = max(len(embedding) for embedding in w2v_train_embeddings)\n",
    "w2v_padded_train_embeddings = pad_sequences(w2v_train_embeddings, maxlen=max_length, padding='post', dtype='float32')\n",
    "w2v_padded_valid_embeddings = pad_sequences(w2v_valid_embeddings, maxlen=max_length, padding='post', dtype='float32')\n",
    "w2v_padded_test_embeddings = pad_sequences(w2v_test_embeddings, maxlen=max_length, padding='post', dtype='float32')\n",
    "\n",
    "\n",
    "# Define the input layer\n",
    "input_text = Input(shape=(max_length, embedding_dim), dtype='float32', name='text_input')\n",
    "\n",
    "# Two LSTM layers\n",
    "x = Bidirectional(LSTM(units=lstm_units, return_sequences=True))(input_text)\n",
    "x = Dropout(0.5)(x)\n",
    "x = Bidirectional(LSTM(units=lstm_units))(embedding)\n",
    "\n",
    "\n",
    "# Output layer\n",
    "output = Dense(num_classes, activation='softmax')(x)\n",
    "\n",
    "# Define the model\n",
    "model_word2vec = Model(inputs=input_text, outputs=output)\n",
    "\n",
    "# Compile the model\n",
    "model_word2vec.compile(optimizer=optimizer, loss=loss, metrics=metrics)"
   ],
   "id": "e077ccd2272c16aa",
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'embedding' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[8], line 16\u001B[0m\n\u001B[1;32m     14\u001B[0m x \u001B[38;5;241m=\u001B[39m Bidirectional(LSTM(units\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m64\u001B[39m, return_sequences\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m))(input_text)\n\u001B[1;32m     15\u001B[0m x \u001B[38;5;241m=\u001B[39m Dropout(\u001B[38;5;241m0.5\u001B[39m)(x)\n\u001B[0;32m---> 16\u001B[0m x \u001B[38;5;241m=\u001B[39m Bidirectional(LSTM(units\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m32\u001B[39m, return_sequences\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m))(\u001B[43membedding\u001B[49m)\n\u001B[1;32m     17\u001B[0m x \u001B[38;5;241m=\u001B[39m Dropout(\u001B[38;5;241m0.5\u001B[39m)(x)\n\u001B[1;32m     18\u001B[0m x \u001B[38;5;241m=\u001B[39m Bidirectional(LSTM(units\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m16\u001B[39m, return_sequences\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m))(embedding)\n",
      "\u001B[0;31mNameError\u001B[0m: name 'embedding' is not defined"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-21T19:44:06.874165Z",
     "start_time": "2024-07-21T19:03:26.370041Z"
    }
   },
   "cell_type": "code",
   "source": [
    "history_word2vec = model_word2vec.fit(w2v_padded_train_embeddings, train_labels, \n",
    "                                      epochs=10, batch_size=32, \n",
    "                                      validation_data=(w2v_padded_valid_embeddings, valid_labels))"
   ],
   "id": "d6567abe87084c13",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-21 21:03:31.992280: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:117] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[1m849/849\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m253s\u001B[0m 290ms/step - accuracy: 0.3981 - auc: 0.7935 - f1_score: 0.1251 - loss: 1.3708 - val_accuracy: 0.4085 - val_auc: 0.8129 - val_f1_score: 0.1545 - val_loss: 1.3150\n",
      "Epoch 2/10\n",
      "\u001B[1m849/849\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m242s\u001B[0m 285ms/step - accuracy: 0.4007 - auc: 0.8069 - f1_score: 0.1287 - loss: 1.3239 - val_accuracy: 0.4075 - val_auc: 0.8172 - val_f1_score: 0.0966 - val_loss: 1.2927\n",
      "Epoch 3/10\n",
      "\u001B[1m849/849\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m243s\u001B[0m 286ms/step - accuracy: 0.4078 - auc: 0.8097 - f1_score: 0.1327 - loss: 1.3138 - val_accuracy: 0.3870 - val_auc: 0.8158 - val_f1_score: 0.1510 - val_loss: 1.2949\n",
      "Epoch 4/10\n",
      "\u001B[1m849/849\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m242s\u001B[0m 285ms/step - accuracy: 0.4166 - auc: 0.8161 - f1_score: 0.1651 - loss: 1.2945 - val_accuracy: 0.4555 - val_auc: 0.8387 - val_f1_score: 0.1920 - val_loss: 1.2226\n",
      "Epoch 5/10\n",
      "\u001B[1m849/849\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m242s\u001B[0m 285ms/step - accuracy: 0.4216 - auc: 0.8191 - f1_score: 0.1630 - loss: 1.2867 - val_accuracy: 0.4578 - val_auc: 0.8414 - val_f1_score: 0.1920 - val_loss: 1.2147\n",
      "Epoch 6/10\n",
      "\u001B[1m849/849\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m243s\u001B[0m 286ms/step - accuracy: 0.4222 - auc: 0.8193 - f1_score: 0.1667 - loss: 1.2859 - val_accuracy: 0.4546 - val_auc: 0.8424 - val_f1_score: 0.2069 - val_loss: 1.2160\n",
      "Epoch 7/10\n",
      "\u001B[1m849/849\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m243s\u001B[0m 286ms/step - accuracy: 0.4371 - auc: 0.8244 - f1_score: 0.1709 - loss: 1.2742 - val_accuracy: 0.4685 - val_auc: 0.8468 - val_f1_score: 0.2120 - val_loss: 1.2030\n",
      "Epoch 8/10\n",
      "\u001B[1m849/849\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m243s\u001B[0m 286ms/step - accuracy: 0.4346 - auc: 0.8245 - f1_score: 0.1869 - loss: 1.2720 - val_accuracy: 0.4795 - val_auc: 0.8434 - val_f1_score: 0.2374 - val_loss: 1.2302\n",
      "Epoch 9/10\n",
      "\u001B[1m849/849\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m242s\u001B[0m 285ms/step - accuracy: 0.4328 - auc: 0.8208 - f1_score: 0.1783 - loss: 1.2828 - val_accuracy: 0.4614 - val_auc: 0.8413 - val_f1_score: 0.2191 - val_loss: 1.2204\n",
      "Epoch 10/10\n",
      "\u001B[1m849/849\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m243s\u001B[0m 286ms/step - accuracy: 0.4317 - auc: 0.8215 - f1_score: 0.1759 - loss: 1.2822 - val_accuracy: 0.4551 - val_auc: 0.8433 - val_f1_score: 0.2188 - val_loss: 1.2083\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Bert Embedding Model",
   "id": "5d5ff88ca032310b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-22T09:57:26.194030Z",
     "start_time": "2024-07-22T09:57:26.176840Z"
    }
   },
   "cell_type": "code",
   "source": [
    "optimizer = Adam()\n",
    "loss = tf.keras.losses.CategoricalCrossentropy()\n",
    "\n",
    "metrics=[\n",
    "      tf.keras.metrics.CategoricalAccuracy(name='accuracy'),\n",
    "      tf.keras.metrics.AUC(name='auc'),\n",
    "      tf.keras.metrics.F1Score(name='f1_score'),\n",
    "]"
   ],
   "id": "2767be12e14abfd6",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-22T09:57:28.790821Z",
     "start_time": "2024-07-22T09:57:27.316975Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import TFBertTokenizer\n",
    "import concurrent.futures\n",
    "\n",
    "# Function to generate word embeddings for a given batch of texts\n",
    "def generate_bert_embeddings_batch(texts, tokenizer, model, max_length):\n",
    "    inputs = tokenizer(texts, truncation=True, padding='max_length', max_length=max_length)\n",
    "    outputs = model(inputs)\n",
    "    embeddings = outputs.last_hidden_state\n",
    "    return embeddings.numpy()\n",
    "\n",
    "model_name = \"GerMedBERT/medbert-512\"\n",
    "tokenizer = TFBertTokenizer.from_pretrained(model_name)\n",
    "model = TFBertModel.from_pretrained(model_name)\n",
    "\n",
    "# Function to generate embeddings concurrently in batches\n",
    "def generate_embeddings_concurrently(texts, tokenizer, model, max_length, batch_size=32):\n",
    "    embeddings = []\n",
    "    with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "        futures = []\n",
    "        for i in range(0, len(texts), batch_size):\n",
    "            batch_texts = texts[i:i+batch_size]\n",
    "            futures.append(executor.submit(generate_bert_embeddings_batch, batch_texts, tokenizer, model, max_length))\n",
    "        \n",
    "        for future in concurrent.futures.as_completed(futures):\n",
    "            try:\n",
    "                batch_embeddings = future.result()\n",
    "                embeddings.extend(batch_embeddings)\n",
    "            except Exception as exc:\n",
    "                print(f'Generated an exception: {exc}')\n",
    "    return embeddings"
   ],
   "id": "5f0fa7f0fd63ccdf",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'bert.embeddings.position_ids', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing TFBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights or buffers of the TF 2.0 model TFBertModel were not initialized from the PyTorch model and are newly initialized: ['bert.pooler.dense.weight', 'bert.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2024-07-22T09:57:30.377751Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pickle\n",
    "\n",
    "\n",
    "# Ensure texts are in list format\n",
    "#train_texts = train_texts.tolist()\n",
    "#valid_texts = valid_texts.tolist()\n",
    "#test_texts = test_texts.tolist()\n",
    "\n",
    "# Define the max length for padding\n",
    "max_length = 400  # or set to a specific value based on your data\n",
    "\n",
    "# Generate embeddings concurrently in batches\n",
    "bert_train_embeddings = generate_embeddings_concurrently(train_texts, tokenizer, model, max_length)\n",
    "bert_valid_embeddings = generate_embeddings_concurrently(valid_texts, tokenizer, model, max_length)\n",
    "bert_test_embeddings = generate_embeddings_concurrently(test_texts, tokenizer, model, max_length)\n",
    "\n",
    "# Pad sequences to the same length\n",
    "bert_padded_train_embeddings = pad_sequences(bert_train_embeddings, padding='post', dtype='float32', maxlen=max_length)\n",
    "bert_padded_valid_embeddings = pad_sequences(bert_valid_embeddings, padding='post', dtype='float32', maxlen=max_length)\n",
    "bert_padded_test_embeddings = pad_sequences(bert_test_embeddings, padding='post', dtype='float32', maxlen=max_length)\n",
    "\n",
    "# Save padded embeddings to files\n",
    "with open('bert_train.pkl', 'wb') as file:\n",
    "    pickle.dump(bert_padded_train_embeddings, file)\n",
    "with open('bert_valid.pkl', 'wb') as file:\n",
    "    pickle.dump(bert_padded_valid_embeddings, file)\n",
    "with open('bert_test.pkl', 'wb') as file:\n",
    "    pickle.dump(bert_padded_test_embeddings, file)"
   ],
   "id": "65852dcea4252d2e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "# Define model parameters\n",
    "lstm_units = 128  # Number of LSTM units\n",
    "embedding_dim = padded_embeddings.shape[-1]\n",
    "\n",
    "# Define the input layers\n",
    "input_text = Input(shape=(padded_embeddings.shape[1], embedding_dim), dtype='float32', name='text_input')\n",
    "\n",
    "# Two LSTM layers\n",
    "x = Bidirectional(LSTM(units=lstm_units, return_sequences=True))(input_text)\n",
    "x = Dropout(0.5)(x)\n",
    "x = Bidirectional(LSTM(units=lstm_units))(embedding)\n",
    "\n",
    "# Output layer\n",
    "output = Dense(num_classes, activation='softmax')(x)\n",
    "\n",
    "# Define the model\n",
    "model_bert = Model(inputs=input_text, outputs=output)\n",
    "\n",
    "# Compile the model\n",
    "model_bert.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Summary of the model\n",
    "model_bert.summary()"
   ],
   "id": "d203e09b8ebb4fb3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "dfb90b47380e0e1b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Evaluation",
   "id": "1f402894d5f16c50"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-21T15:37:16.915694Z",
     "start_time": "2024-07-21T15:37:16.915633Z"
    }
   },
   "cell_type": "code",
   "source": [
    "eval_training(history_base, metrics)\n",
    "eval(model_base, test_padded, test_labels, label_encoder)"
   ],
   "id": "61158811156cf4c5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "eval_training(history_word2vec, metrics)\n",
    "eval(model_word2vec, w2v_padded_test_embeddings, test_labels, label_encoder)"
   ],
   "id": "6d9f19a3b7920202",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "7f8af8f1ad931301"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
