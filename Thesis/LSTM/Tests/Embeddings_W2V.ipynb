{
 "cells": [
  {
   "cell_type": "code",
   "id": "1fdc6db225373a11",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-15T21:51:29.634219Z",
     "start_time": "2024-07-15T21:51:28.456452Z"
    }
   },
   "source": [
    "import re\n",
    "from datasets import load_dataset\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from gensim.models import Word2Vec\n",
    "import numpy as np\n",
    "import nltk\n",
    "from datasets import load_dataset\n",
    "import pickle\n",
    "\n",
    "# Download required NLTK data\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/juliankraus/Library/CloudStorage/OneDrive-Personal/Uni/Languages/Code/pythonProject/.venv/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/juliankraus/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/juliankraus/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8fee3eedb880c71e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-12T10:12:49.904237Z",
     "start_time": "2024-07-12T10:12:45.060653Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e36e866eddc4ed4aa4687ffbd566400",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset shards:   0%|          | 0/18 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# https://huggingface.co/datasets/legacy-datasets/wikipedia\n",
    "dataset = load_dataset(\"wikipedia\", \"20220301.de\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6ff418ec28a071aa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-12T10:43:55.826993Z",
     "start_time": "2024-07-12T10:43:55.823824Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of examples: 2665357\n"
     ]
    }
   ],
   "source": [
    "num_examples = len(dataset['train'])\n",
    "print(f\"Number of examples: {num_examples}\")\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Cleaning",
   "id": "719afbfc75a526f1"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d5cbf1f3d19e6c82",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-12T13:29:23.185715Z",
     "start_time": "2024-07-12T13:26:20.342631Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/juliankraus/Library/CloudStorage/OneDrive-Personal/Uni/Languages/Code/pythonProject/.venv/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5da6e9f61b0d4b9d9d00b84141403c98",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset shards:   0%|          | 0/18 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "NameError",
     "evalue": "name 'pickle' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[1], line 26\u001B[0m\n\u001B[1;32m     24\u001B[0m brief_cleaning \u001B[38;5;241m=\u001B[39m [re\u001B[38;5;241m.\u001B[39msub(\u001B[38;5;124mr\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m[^a-zA-ZäöüÄÖÜß\u001B[39m\u001B[38;5;124m\\\u001B[39m\u001B[38;5;124ms]\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m'\u001B[39m, text)\u001B[38;5;241m.\u001B[39mlower() \u001B[38;5;28;01mfor\u001B[39;00m text \u001B[38;5;129;01min\u001B[39;00m texts]\n\u001B[1;32m     25\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mopen\u001B[39m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mcleaned_texts.pkl\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mwb\u001B[39m\u001B[38;5;124m'\u001B[39m) \u001B[38;5;28;01mas\u001B[39;00m file:\n\u001B[0;32m---> 26\u001B[0m     \u001B[43mpickle\u001B[49m\u001B[38;5;241m.\u001B[39mdump(brief_cleaning, file)\n",
      "\u001B[0;31mNameError\u001B[0m: name 'pickle' is not defined"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import spacy\n",
    "from datasets import load_dataset\n",
    "from time import time\n",
    "import os\n",
    "import multiprocessing\n",
    "\n",
    "# Load SpaCy model for German\n",
    "nlp = spacy.load('de_core_news_sm')\n",
    "\n",
    "# Load the dataset\n",
    "dataset = load_dataset(\"wikipedia\", \"20220301.de\")\n",
    "\n",
    "# Preprocessing function\n",
    "def preprocess_text(doc):\n",
    "    stop_words = spacy.lang.de.stop_words.STOP_WORDS\n",
    "    words = [token.text for token in doc if token.text not in stop_words and token.is_alpha]\n",
    "    return words\n",
    "\n",
    "# Prepare the texts for processing\n",
    "texts = [article['text'] for article in dataset['train']]\n",
    "\n",
    "# Brief cleaning to remove non-alphabetic characters\n",
    "brief_cleaning = [re.sub(r'[^a-zA-ZäöüÄÖÜß\\s]', '', text).lower() for text in texts]\n",
    "with open('cleaned_texts.pkl', 'wb') as file:\n",
    "    pickle.dump(brief_cleaning, file)\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Preprocessing",
   "id": "6f14c8582f996590"
  },
  {
   "cell_type": "code",
   "id": "38d871fb75d18736",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-15T01:46:41.823140Z",
     "start_time": "2024-07-14T22:28:54.535125Z"
    }
   },
   "source": [
    "from keras.src.layers import LSTM, Dense\n",
    "from keras import Sequential, Input\n",
    "from keras.src.utils import pad_sequences\n",
    "import pickle\n",
    "import re\n",
    "import spacy\n",
    "from datasets import load_dataset\n",
    "from time import time\n",
    "import os\n",
    "def preprocess_text(doc):\n",
    "    stop_words = spacy.lang.de.stop_words.STOP_WORDS\n",
    "    words = [token.text for token in doc if token.text not in stop_words and token.is_alpha]\n",
    "    return words\n",
    "nlp = spacy.load('de_core_news_sm', disable=['parser', 'ner'])\n",
    "# Load the cleaned texts from the pickle file\n",
    "with open('cleaned_texts.pkl', 'rb') as file:\n",
    "    brief_cleaning = pickle.load(file)\n",
    "# Measure time for processing\n",
    "start_time = time()\n",
    "num_cores = os.cpu_count()\n",
    "print(f\"Number of CPU cores: {num_cores}\")\n",
    "\n",
    "# Use spacy.pipe for concurrent processing\n",
    "# Leave 1 or 2 cores free (for example, use num_cores - 1)\n",
    "n_process = max(1, num_cores - 1)\n",
    "# Use spacy.pipe for concurrent processing\n",
    "processed_texts = [preprocess_text(doc) for doc in nlp.pipe(brief_cleaning, batch_size=100, n_process=10)]\n",
    "\n",
    "# Measure and print the time taken for processing\n",
    "end_time = time()\n",
    "print(f\"Time to clean up everything: {round((end_time - start_time) / 60, 2)} mins\")\n",
    "\n",
    "# Now `processed_texts` contains the cleaned and preprocessed texts\n",
    "print(f\"Processed {len(processed_texts)} articles.\")\n",
    "\n",
    "import pickle \n",
    "with open('database_preprocessed.pkl', 'wb') as file: \n",
    "    # A new file will be created \n",
    "    pickle.dump(processed_texts, file) "
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/juliankraus/Library/CloudStorage/OneDrive-Personal/Uni/Languages/Code/pythonProject/.venv/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of CPU cores: 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/juliankraus/Library/CloudStorage/OneDrive-Personal/Uni/Languages/Code/pythonProject/.venv/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n",
      "/Users/juliankraus/Library/CloudStorage/OneDrive-Personal/Uni/Languages/Code/pythonProject/.venv/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n",
      "/Users/juliankraus/Library/CloudStorage/OneDrive-Personal/Uni/Languages/Code/pythonProject/.venv/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n",
      "/Users/juliankraus/Library/CloudStorage/OneDrive-Personal/Uni/Languages/Code/pythonProject/.venv/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n",
      "/Users/juliankraus/Library/CloudStorage/OneDrive-Personal/Uni/Languages/Code/pythonProject/.venv/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n",
      "/Users/juliankraus/Library/CloudStorage/OneDrive-Personal/Uni/Languages/Code/pythonProject/.venv/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n",
      "/Users/juliankraus/Library/CloudStorage/OneDrive-Personal/Uni/Languages/Code/pythonProject/.venv/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n",
      "/Users/juliankraus/Library/CloudStorage/OneDrive-Personal/Uni/Languages/Code/pythonProject/.venv/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n",
      "/Users/juliankraus/Library/CloudStorage/OneDrive-Personal/Uni/Languages/Code/pythonProject/.venv/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n",
      "/Users/juliankraus/Library/CloudStorage/OneDrive-Personal/Uni/Languages/Code/pythonProject/.venv/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to clean up everything: 194.23 mins\n",
      "Processed 2665357 articles.\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Word2Vec",
   "id": "5fecd777cf58ddc8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Training the Word2Vec model\n",
    "model = Word2Vec(\n",
    "    sentences=processed_texts,\n",
    "    vector_size=300,  # Dimensionality of the word vectors\n",
    "    window=8,        # Maximum distance between the current and predicted word within a sentence\n",
    "    min_count=5,     # Ignores all words with total frequency lower than this\n",
    "    workers=10,       # Use these many worker threads to train the model\n",
    "    sg=1,            # Training algorithm: 1 for skip-gram; otherwise CBOW\n",
    "    epochs=10        # Number of iterations (epochs) over the corpus\n",
    ")\n",
    "model.save(\"word2vec.model\")\n",
    "import pickle \n",
    "with open('w2v.pkl', 'wb') as file: \n",
    "    pickle.dump(model, file) "
   ],
   "id": "2a984829bd0a05b7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import re\n",
    "import spacy\n",
    "from datasets import load_dataset\n",
    "from time import time\n",
    "import os\n",
    "import multiprocessing\n",
    "\n",
    "# Load SpaCy model for German\n",
    "nlp = spacy.load('de_core_news_sm')\n",
    "# Function to get word vectors for a given text\n",
    "def preprocess_text(text):\n",
    "    # Remove non-alphabetic characters\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    doc = nlp(text)\n",
    "    stop_words = spacy.lang.de.stop_words.STOP_WORDS\n",
    "    words = [token.text for token in doc if token.text not in stop_words and token.is_alpha]\n",
    "    return words\n",
    "def get_word2vec_embeddings(text, model, vector_size=300):\n",
    "    tokens = preprocess_text(text)\n",
    "    embeddings = np.zeros((len(tokens), vector_size))\n",
    "    for i, token in enumerate(tokens):\n",
    "        if token in model.wv:\n",
    "            embeddings[i] = model.wv[token]\n",
    "        else:\n",
    "            embeddings[i] = np.zeros(vector_size)\n",
    "    return embeddings\n",
    "\n",
    "# Example text\n",
    "text = \"April is the fourth month of the year.\"\n",
    "word2vec_embeddings = get_word2vec_embeddings(text, model)\n",
    "print(word2vec_embeddings.shape)"
   ],
   "id": "ff5dca3614c96c5d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# LSTM;",
   "id": "4909a7988fe60a54"
  },
  {
   "cell_type": "code",
   "id": "fd3ab405be036242",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-16T08:28:56.811524Z",
     "start_time": "2024-07-16T08:28:51.405890Z"
    }
   },
   "source": [
    "from keras.src.layers import LSTM, Dense\n",
    "from keras import Sequential, Input\n",
    "from keras.src.utils import pad_sequences\n",
    "# Example dataset\n",
    "texts = [\"April is the fourth month of the year.\", \"May is the fifth month of the year.\"]\n",
    "labels = [0, 1]\n",
    "\n",
    "# Convert texts to embeddings\n",
    "word2vec_embeddings = [get_word2vec_embeddings(text, model) for text in texts]\n",
    "padded_embeddings = pad_sequences(word2vec_embeddings, padding='post', dtype='float32')\n",
    "\n",
    "# LSTM model\n",
    "model = Sequential([\n",
    "    Input(shape=(padded_embeddings.shape[1], 100)),\n",
    "    LSTM(128),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "# Train the model\n",
    "model.fit(padded_embeddings, labels, epochs=10)"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001B[1mModel: \"sequential\"\u001B[0m\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001B[1m \u001B[0m\u001B[1mLayer (type)                   \u001B[0m\u001B[1m \u001B[0m┃\u001B[1m \u001B[0m\u001B[1mOutput Shape          \u001B[0m\u001B[1m \u001B[0m┃\u001B[1m \u001B[0m\u001B[1m      Param #\u001B[0m\u001B[1m \u001B[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ lstm (\u001B[38;5;33mLSTM\u001B[0m)                     │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m128\u001B[0m)            │       \u001B[38;5;34m117,248\u001B[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001B[38;5;33mDense\u001B[0m)                   │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m1\u001B[0m)              │           \u001B[38;5;34m129\u001B[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">117,248</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">129</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001B[1m Total params: \u001B[0m\u001B[38;5;34m117,377\u001B[0m (458.50 KB)\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">117,377</span> (458.50 KB)\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001B[1m Trainable params: \u001B[0m\u001B[38;5;34m117,377\u001B[0m (458.50 KB)\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">117,377</span> (458.50 KB)\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001B[1m Non-trainable params: \u001B[0m\u001B[38;5;34m0\u001B[0m (0.00 B)\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ValueError",
     "evalue": "Unrecognized data type: x=[[[ 0.          0.          0.         ...  0.          0.\n    0.        ]\n  [-0.00992339 -0.00465845  0.06648266 ...  0.02166415 -0.20440356\n   -0.1717809 ]\n  [ 0.15835191 -0.24108157 -0.13057461 ...  0.08755513  0.17822938\n    0.08964378]\n  ...\n  [ 0.2503722  -0.31800562  0.15515076 ...  0.22381121  0.07700533\n    0.04427565]\n  [ 0.15835191 -0.24108157 -0.13057461 ...  0.08755513  0.17822938\n    0.08964378]\n  [ 0.42727315  0.03994293 -0.02188285 ...  0.37655184  0.57135147\n   -0.26862139]]\n\n [[ 0.          0.          0.         ...  0.          0.\n    0.        ]\n  [-0.00992339 -0.00465845  0.06648266 ...  0.02166415 -0.20440356\n   -0.1717809 ]\n  [ 0.15835191 -0.24108157 -0.13057461 ...  0.08755513  0.17822938\n    0.08964378]\n  ...\n  [ 0.2503722  -0.31800562  0.15515076 ...  0.22381121  0.07700533\n    0.04427565]\n  [ 0.15835191 -0.24108157 -0.13057461 ...  0.08755513  0.17822938\n    0.08964378]\n  [ 0.42727315  0.03994293 -0.02188285 ...  0.37655184  0.57135147\n   -0.26862139]]] (of type <class 'numpy.ndarray'>)",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[9], line 23\u001B[0m\n\u001B[1;32m     20\u001B[0m model\u001B[38;5;241m.\u001B[39msummary()\n\u001B[1;32m     22\u001B[0m \u001B[38;5;66;03m# Train the model\u001B[39;00m\n\u001B[0;32m---> 23\u001B[0m \u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpadded_embeddings\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlabels\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mepochs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m10\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Library/CloudStorage/OneDrive-Personal/Uni/Languages/Code/pythonProject/.venv/lib/python3.9/site-packages/keras/src/utils/traceback_utils.py:122\u001B[0m, in \u001B[0;36mfilter_traceback.<locals>.error_handler\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    119\u001B[0m     filtered_tb \u001B[38;5;241m=\u001B[39m _process_traceback_frames(e\u001B[38;5;241m.\u001B[39m__traceback__)\n\u001B[1;32m    120\u001B[0m     \u001B[38;5;66;03m# To get the full stack trace, call:\u001B[39;00m\n\u001B[1;32m    121\u001B[0m     \u001B[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001B[39;00m\n\u001B[0;32m--> 122\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m e\u001B[38;5;241m.\u001B[39mwith_traceback(filtered_tb) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m    123\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[1;32m    124\u001B[0m     \u001B[38;5;28;01mdel\u001B[39;00m filtered_tb\n",
      "File \u001B[0;32m~/Library/CloudStorage/OneDrive-Personal/Uni/Languages/Code/pythonProject/.venv/lib/python3.9/site-packages/keras/src/trainers/data_adapters/__init__.py:120\u001B[0m, in \u001B[0;36mget_data_adapter\u001B[0;34m(x, y, sample_weight, batch_size, steps_per_epoch, shuffle, class_weight)\u001B[0m\n\u001B[1;32m    112\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m GeneratorDataAdapter(x)\n\u001B[1;32m    113\u001B[0m     \u001B[38;5;66;03m# TODO: should we warn or not?\u001B[39;00m\n\u001B[1;32m    114\u001B[0m     \u001B[38;5;66;03m# warnings.warn(\u001B[39;00m\n\u001B[1;32m    115\u001B[0m     \u001B[38;5;66;03m#     \"`shuffle=True` was passed, but will be ignored since the \"\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    118\u001B[0m     \u001B[38;5;66;03m# )\u001B[39;00m\n\u001B[1;32m    119\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 120\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mUnrecognized data type: x=\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mx\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m (of type \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mtype\u001B[39m(x)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m)\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "\u001B[0;31mValueError\u001B[0m: Unrecognized data type: x=[[[ 0.          0.          0.         ...  0.          0.\n    0.        ]\n  [-0.00992339 -0.00465845  0.06648266 ...  0.02166415 -0.20440356\n   -0.1717809 ]\n  [ 0.15835191 -0.24108157 -0.13057461 ...  0.08755513  0.17822938\n    0.08964378]\n  ...\n  [ 0.2503722  -0.31800562  0.15515076 ...  0.22381121  0.07700533\n    0.04427565]\n  [ 0.15835191 -0.24108157 -0.13057461 ...  0.08755513  0.17822938\n    0.08964378]\n  [ 0.42727315  0.03994293 -0.02188285 ...  0.37655184  0.57135147\n   -0.26862139]]\n\n [[ 0.          0.          0.         ...  0.          0.\n    0.        ]\n  [-0.00992339 -0.00465845  0.06648266 ...  0.02166415 -0.20440356\n   -0.1717809 ]\n  [ 0.15835191 -0.24108157 -0.13057461 ...  0.08755513  0.17822938\n    0.08964378]\n  ...\n  [ 0.2503722  -0.31800562  0.15515076 ...  0.22381121  0.07700533\n    0.04427565]\n  [ 0.15835191 -0.24108157 -0.13057461 ...  0.08755513  0.17822938\n    0.08964378]\n  [ 0.42727315  0.03994293 -0.02188285 ...  0.37655184  0.57135147\n   -0.26862139]]] (of type <class 'numpy.ndarray'>)"
     ]
    }
   ],
   "execution_count": 9
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
