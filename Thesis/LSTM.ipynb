{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-06-11T14:23:18.174615Z",
     "start_time": "2024-06-11T14:23:16.144995Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Bidirectional, BatchNormalization\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import IO\n",
    "from keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/juliankraus/Library/CloudStorage/OneDrive-Personal/Uni/Languages/Code/pythonProject/.venv/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-11T14:23:18.179913Z",
     "start_time": "2024-06-11T14:23:18.175565Z"
    }
   },
   "cell_type": "code",
   "source": "X_train, X_test, y_train, y_test, X_train_pad, X_test_pad, y_train_cat, y_test_cat, _ = IO.load_training_data(\"MenKreuz\")",
   "id": "d967c0a405642bb1",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-11T14:23:18.182081Z",
     "start_time": "2024-06-11T14:23:18.180623Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# # Define the model\n",
    "# model = Sequential()\n",
    "# model.add(Embedding(input_dim=10000, output_dim=128, input_length=100))\n",
    "# model.add(Bidirectional(LSTM(units=128, return_sequences=True)))\n",
    "# model.add(Dropout(0.5))\n",
    "# model.add(Bidirectional(LSTM(units=64)))\n",
    "# model.add(Dense(units=2, activation='softmax'))\n",
    "# \n",
    "# model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ],
   "id": "d33bcbf4e7fb6050",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-11T14:25:41.977654Z",
     "start_time": "2024-06-11T14:25:41.910502Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, Embedding, Input, LayerNormalization, Dropout, GlobalAveragePooling1D\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Transformer parameters\n",
    "maxlen = 100\n",
    "vocab_size = 10000\n",
    "embed_dim = 128\n",
    "num_heads = 4\n",
    "ff_dim = 128\n",
    "dropout_rate = 0.5\n",
    "num_classes = 2\n",
    "\n",
    "class MultiHeadSelfAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads=8):\n",
    "        super(MultiHeadSelfAttention, self).__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.projection_dim = embed_dim // num_heads\n",
    "        self.query_dense = Dense(embed_dim)\n",
    "        self.key_dense = Dense(embed_dim)\n",
    "        self.value_dense = Dense(embed_dim)\n",
    "        self.combine_heads = Dense(embed_dim)\n",
    "\n",
    "    def attention(self, query, key, value):\n",
    "        score = tf.matmul(query, key, transpose_b=True)\n",
    "        dim_key = tf.cast(tf.shape(key)[-1], tf.float32)\n",
    "        scaled_score = score / tf.math.sqrt(dim_key)\n",
    "        weights = tf.nn.softmax(scaled_score, axis=-1)\n",
    "        output = tf.matmul(weights, value)\n",
    "        return output, weights\n",
    "\n",
    "    def separate_heads(self, x, batch_size):\n",
    "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.projection_dim))\n",
    "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "\n",
    "    def call(self, inputs):\n",
    "        batch_size = tf.shape(inputs)[0]\n",
    "        query = self.query_dense(inputs)\n",
    "        key = self.key_dense(inputs)\n",
    "        value = self.value_dense(inputs)\n",
    "        query = self.separate_heads(query, batch_size)\n",
    "        key = self.separate_heads(key, batch_size)\n",
    "        value = self.separate_heads(value, batch_size)\n",
    "        attention, weights = self.attention(query, key, value)\n",
    "        attention = tf.transpose(attention, perm=[0, 2, 1, 3])\n",
    "        concat_attention = tf.reshape(attention, (batch_size, -1, self.embed_dim))\n",
    "        output = self.combine_heads(concat_attention)\n",
    "        return output\n",
    "\n",
    "class TransformerBlock(tf.keras.layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.att = MultiHeadSelfAttention(embed_dim, num_heads)\n",
    "        self.ffn = tf.keras.Sequential(\n",
    "            [Dense(ff_dim, activation=\"relu\"), Dense(embed_dim),]\n",
    "        )\n",
    "        self.layernorm1 = LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = Dropout(rate)\n",
    "        self.dropout2 = Dropout(rate)\n",
    "\n",
    "    def call(self, inputs, training):\n",
    "        attn_output = self.att(inputs)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(inputs + attn_output)\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        return self.layernorm2(out1 + ffn_output)\n",
    "\n",
    "def build_transformer_model():\n",
    "    inputs = Input(shape=(maxlen,))\n",
    "    embedding_layer = Embedding(vocab_size, embed_dim)(inputs)\n",
    "    transformer_block = TransformerBlock(embed_dim, num_heads, ff_dim, dropout_rate)\n",
    "    x = transformer_block(embedding_layer)\n",
    "    x = GlobalAveragePooling1D()(x)\n",
    "    x = Dropout(dropout_rate)(x)\n",
    "    x = Dense(20, activation=\"relu\")(x)\n",
    "    x = Dropout(dropout_rate)(x)\n",
    "    outputs = Dense(num_classes, activation=\"softmax\")(x)\n",
    "\n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    return model\n",
    "\n",
    "# Build and compile the Transformer model\n",
    "transformer_model = build_transformer_model()\n",
    "transformer_model.compile(optimizer=Adam(), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Print the model summary\n",
    "transformer_model.summary()\n"
   ],
   "id": "892106cfd117a00b",
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "missing a required argument: 'training'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[5], line 87\u001B[0m\n\u001B[1;32m     84\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m model\n\u001B[1;32m     86\u001B[0m \u001B[38;5;66;03m# Build and compile the Transformer model\u001B[39;00m\n\u001B[0;32m---> 87\u001B[0m transformer_model \u001B[38;5;241m=\u001B[39m \u001B[43mbuild_transformer_model\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     88\u001B[0m transformer_model\u001B[38;5;241m.\u001B[39mcompile(optimizer\u001B[38;5;241m=\u001B[39mAdam(), loss\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mcategorical_crossentropy\u001B[39m\u001B[38;5;124m'\u001B[39m, metrics\u001B[38;5;241m=\u001B[39m[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124maccuracy\u001B[39m\u001B[38;5;124m'\u001B[39m])\n\u001B[1;32m     90\u001B[0m \u001B[38;5;66;03m# Print the model summary\u001B[39;00m\n",
      "Cell \u001B[0;32mIn[5], line 76\u001B[0m, in \u001B[0;36mbuild_transformer_model\u001B[0;34m()\u001B[0m\n\u001B[1;32m     74\u001B[0m embedding_layer \u001B[38;5;241m=\u001B[39m Embedding(vocab_size, embed_dim)(inputs)\n\u001B[1;32m     75\u001B[0m transformer_block \u001B[38;5;241m=\u001B[39m TransformerBlock(embed_dim, num_heads, ff_dim, dropout_rate)\n\u001B[0;32m---> 76\u001B[0m x \u001B[38;5;241m=\u001B[39m \u001B[43mtransformer_block\u001B[49m\u001B[43m(\u001B[49m\u001B[43membedding_layer\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     77\u001B[0m x \u001B[38;5;241m=\u001B[39m GlobalAveragePooling1D()(x)\n\u001B[1;32m     78\u001B[0m x \u001B[38;5;241m=\u001B[39m Dropout(dropout_rate)(x)\n",
      "File \u001B[0;32m~/Library/CloudStorage/OneDrive-Personal/Uni/Languages/Code/pythonProject/.venv/lib/python3.9/site-packages/keras/src/utils/traceback_utils.py:122\u001B[0m, in \u001B[0;36mfilter_traceback.<locals>.error_handler\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    119\u001B[0m     filtered_tb \u001B[38;5;241m=\u001B[39m _process_traceback_frames(e\u001B[38;5;241m.\u001B[39m__traceback__)\n\u001B[1;32m    120\u001B[0m     \u001B[38;5;66;03m# To get the full stack trace, call:\u001B[39;00m\n\u001B[1;32m    121\u001B[0m     \u001B[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001B[39;00m\n\u001B[0;32m--> 122\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m e\u001B[38;5;241m.\u001B[39mwith_traceback(filtered_tb) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m    123\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[1;32m    124\u001B[0m     \u001B[38;5;28;01mdel\u001B[39;00m filtered_tb\n",
      "File \u001B[0;32m/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/inspect.py:3062\u001B[0m, in \u001B[0;36mSignature.bind\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   3057\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mbind\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m/\u001B[39m, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[1;32m   3058\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Get a BoundArguments object, that maps the passed `args`\u001B[39;00m\n\u001B[1;32m   3059\u001B[0m \u001B[38;5;124;03m    and `kwargs` to the function's signature.  Raises `TypeError`\u001B[39;00m\n\u001B[1;32m   3060\u001B[0m \u001B[38;5;124;03m    if the passed arguments can not be bound.\u001B[39;00m\n\u001B[1;32m   3061\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m-> 3062\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_bind\u001B[49m\u001B[43m(\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/inspect.py:2977\u001B[0m, in \u001B[0;36mSignature._bind\u001B[0;34m(self, args, kwargs, partial)\u001B[0m\n\u001B[1;32m   2975\u001B[0m                 msg \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mmissing a required argument: \u001B[39m\u001B[38;5;132;01m{arg!r}\u001B[39;00m\u001B[38;5;124m'\u001B[39m\n\u001B[1;32m   2976\u001B[0m                 msg \u001B[38;5;241m=\u001B[39m msg\u001B[38;5;241m.\u001B[39mformat(arg\u001B[38;5;241m=\u001B[39mparam\u001B[38;5;241m.\u001B[39mname)\n\u001B[0;32m-> 2977\u001B[0m                 \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m(msg) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   2978\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m   2979\u001B[0m     \u001B[38;5;66;03m# We have a positional argument to process\u001B[39;00m\n\u001B[1;32m   2980\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n",
      "\u001B[0;31mTypeError\u001B[0m: missing a required argument: 'training'"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-11T14:23:18.576852Z",
     "start_time": "2024-06-11T14:23:18.576798Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# from keras.models import Sequential\n",
    "# from keras.layers import Embedding, LSTM, Dropout, Dense, BatchNormalization, Bidirectional\n",
    "# from keras.losses import BinaryCrossentropy\n",
    "# from keras.optimizers import Adam\n",
    "# from keras.regularizers import l2\n",
    "# from keras.callbacks import ReduceLROnPlateau\n",
    "# \n",
    "# # Define the model\n",
    "# model = Sequential([\n",
    "#     Embedding(input_dim=10000, output_dim=128, input_length=100),\n",
    "#     Bidirectional(LSTM(128, return_sequences=True)),\n",
    "#     BatchNormalization(),  # Batch Normalization to stabilize learning\n",
    "#     Dropout(0.5),\n",
    "#     Bidirectional(LSTM(128)),\n",
    "#     BatchNormalization(),\n",
    "#     Dropout(0.5),\n",
    "#     Dense(64, activation='relu', kernel_regularizer=l2(0.001)),  # Dense layer with L2 regularization\n",
    "#     Dropout(0.5),\n",
    "#     Dense(2, activation='softmax')  # Output layer\n",
    "# ])\n",
    "# \n",
    "# # Compile the model\n",
    "# model.compile(\n",
    "#     loss=BinaryCrossentropy(from_logits=True),\n",
    "#     optimizer=Adam(1e-4),\n",
    "#     metrics=['accuracy']\n",
    "# )\n",
    "# \n",
    "# # Learning rate scheduler callback\n",
    "# lr_scheduler = ReduceLROnPlateau(\n",
    "#     monitor='val_loss',\n",
    "#     factor=0.5,\n",
    "#     patience=3,\n",
    "#     min_lr=1e-6,\n",
    "#     verbose=1\n",
    "# )"
   ],
   "id": "6e8ff448677e4739",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "history = model.fit(X_train_pad, y_train_cat, epochs=30, batch_size=32, validation_data=(X_test_pad, y_test_cat))\n",
   "id": "f05160cae6dd5c1b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "loss, accuracy = model.evaluate(X_test_pad, y_test_cat)\n",
    "print(f'Test Loss: {loss}')\n",
    "print(f'Test Accuracy: {accuracy}')\n"
   ],
   "id": "548cc3fee17c70d1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "IO.save_model(model, history, 'lstm', \"transformer\")",
   "id": "5b743c92ce31385",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "fc1f2ab2f36669cb",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
